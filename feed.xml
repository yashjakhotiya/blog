<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://yashjakhotiya.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yashjakhotiya.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-12-22T14:41:00-06:00</updated><id>https://yashjakhotiya.github.io/blog/feed.xml</id><title type="html">Blog</title><subtitle>Blog</subtitle><entry><title type="html">Containers, Container Runtimes, and What Kubernetes ‘Docker’ Deprecation Really Means</title><link href="https://yashjakhotiya.github.io/blog/containers/kubernetes/2020/12/20/container-runtimes.html" rel="alternate" type="text/html" title="Containers, Container Runtimes, and What Kubernetes 'Docker' Deprecation Really Means" /><published>2020-12-20T00:00:00-06:00</published><updated>2020-12-20T00:00:00-06:00</updated><id>https://yashjakhotiya.github.io/blog/containers/kubernetes/2020/12/20/container-runtimes</id><content type="html" xml:base="https://yashjakhotiya.github.io/blog/containers/kubernetes/2020/12/20/container-runtimes.html">&lt;h1 id=&quot;containers-arent-they--like-some-sort-of-virtual-machines&quot;&gt;Containers? Aren’t they … like some sort of virtual machines?&lt;/h1&gt;

&lt;p&gt;Docker popularized the notion of using containers - &lt;em&gt;isolated environments leveraging OS-level virtualization&lt;/em&gt; where each running process sees the environment as one whole computer. This seems awfully similar to virtual machines, except that it isn’t. Containers differ from virtual machines in that each container does not host the entire operating system the way virtual machines do.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/containers_vs_vms.png&quot; alt=&quot;&quot; title=&quot;Virtual Machines Vs Containers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Container images&lt;/strong&gt;, which become running containers when instantiated, store the application code and any required dependencies mentioned in the image &lt;a href=&quot;https://docs.docker.com/engine/reference/builder/&quot;&gt;Dockerfile&lt;/a&gt;. When you don’t need to worry about dependencies, shipping applications from a developer’s laptop to production servers or public cloud environments becomes easier. You &lt;em&gt;could&lt;/em&gt; package your application as a custom built VM image relying on a fully functional traditional OS packaged with it, but container images are much more lightweight and can be easily maintained.&lt;/p&gt;

&lt;h1 id=&quot;but-every-dockerfile-i-see-ultimately-stems-from-an-os-image-doesnt-that-mean-container-images-have-their-own-os-installed&quot;&gt;But, every dockerfile I see ultimately stems from an OS image. Doesn’t that mean container images have their own OS installed?&lt;/h1&gt;

&lt;p&gt;This is an excellent question. Thanks for asking! To answer this, let us get some background context.&lt;/p&gt;

&lt;p&gt;In most modern OS, an application process runs in what is known as a &lt;strong&gt;user mode&lt;/strong&gt;. The idea here is to restrict the memory area accessible to an application process and prevent it from accessing and potentially corrupting memory areas associated with kernel and other application processes. This is implemented using &lt;a href=&quot;https://en.wikipedia.org/wiki/Virtual_memory&quot;&gt;Virtual memory&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Protection_ring&quot;&gt;Protection Rings&lt;/a&gt;, and assisted by hardware in the form of &lt;a href=&quot;en.wikipedia.org/wiki/Protected_mode&quot;&gt;Protected mode&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Memory_management_unit&quot;&gt;Memory Management Units&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Providing fault tolerance and computer security with this form of &lt;strong&gt;memory protection&lt;/strong&gt; effectively results in a typical OS being divided into two bifurcations - a user space and a kernel space. An application running in user space can access resources which it does not have direct access to (like I/O devices or files lying on a disk) with special requests to the kernel called &lt;a href=&quot;https://man7.org/linux/man-pages/man2/syscalls.2.html&quot;&gt;system calls&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/user_space_kernel_space.png&quot; alt=&quot;&quot; title=&quot;A process in user space makes a system call&quot; /&gt;&lt;/p&gt;

&lt;p&gt;System calls serve as APIs for all &lt;strong&gt;userland&lt;/strong&gt; software to interact with the kernel. In the Linux world, all distros (a bit simplification here) run the same kernel. This makes it possible for the &lt;em&gt;userland software coming from Ubuntu to talk to a CentOS kernel&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;What you see inside a Dockerfile, which gets installed in the built image, is not a full-fledged OS. It is the trimmed-down version of the userland software of the OS, bare enough to talk to the host’s kernel. It is not uncommon to see containers with Ubuntu, CentOS, and Debian base run parallely on a RHEL7 host.&lt;/p&gt;

&lt;h1 id=&quot;ok-i-am-curious-how-is-this-implemented&quot;&gt;Ok. I am curious. How is this implemented?&lt;/h1&gt;

&lt;p&gt;To be honest, container implementation recipe is really not that difficult if you understand its three main ingredients - cgroups, namespaces and chroot. Let us focus on each of them below.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;cgroups&lt;/strong&gt;, or Control Groups is a Linux kernel feature. With cgroups you can &lt;em&gt;allocate, monitor, and limit resources&lt;/em&gt; - like CPU time, memory, or network bandwidth - to a process or a collection of processes. Linux command &lt;a href=&quot;https://linux.die.net/man/1/cgcreate&quot;&gt;cgcreate&lt;/a&gt; helps you create a control group, &lt;a href=&quot;https://linux.die.net/man/1/cgset&quot;&gt;cgset&lt;/a&gt; sets resource limits for the control group, and with &lt;a href=&quot;https://linux.die.net/man/1/cgexec&quot;&gt;cgexec&lt;/a&gt; you can run a command in the control group.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;namespace&lt;/strong&gt; is another Linux kernel feature, with which you can &lt;em&gt;isolate a global resource&lt;/em&gt;. This creates an illusion of a separate instance of the resource to the processes running in the namespace, and any changes made are &lt;strong&gt;not visible outside&lt;/strong&gt;! The resources you can abstract this way include process IDs, hostnames, user IDs, etc. &lt;a href=&quot;https://man7.org/linux/man-pages/man1/unshare.1.html&quot;&gt;unshare&lt;/a&gt; [options] [&lt;em&gt;program&lt;/em&gt; [arguments]] is a Linux utility you can use to create namespaces (supplied in &lt;code class=&quot;highlighter-rouge&quot;&gt;options&lt;/code&gt;) and run &lt;code class=&quot;highlighter-rouge&quot;&gt;program&lt;/code&gt; in it. For example you can create a UTS (Unix Time Sharing) namespace, which controls host and domain names, using the -u option as illustrated below.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; hostname                              # show current hostname 
personal-ubuntu
&amp;gt; unshare -u /bin/sh                    # run a shell instance with UTS namespace unshared from parent
&amp;gt; hostname a-different-hostname         # change hostname to a-different-hostname
&amp;gt; hostname                              # verify that the hostname has been changed
a-different-hostname
&amp;gt; exit                                  # exit from the shell process, effectively destroying the namespace
&amp;gt; hostname                              # voila!
personal-ubuntu                         # changing the hostname inside the namespace has no effect outside!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://linux.die.net/man/1/chroot&quot;&gt;chroot&lt;/a&gt; is a Linux utility that can &lt;em&gt;change the apparent root directory for a process and its children&lt;/em&gt;. Running &lt;code class=&quot;highlighter-rouge&quot;&gt;chroot NEWROOT command&lt;/code&gt; will run &lt;code class=&quot;highlighter-rouge&quot;&gt;command&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;NEWROOT&lt;/code&gt; as its apparent root directory. This modified environment is also called a &lt;strong&gt;chroot jail&lt;/strong&gt;, because &lt;code class=&quot;highlighter-rouge&quot;&gt;command&lt;/code&gt; can not name and hence can not normally access files outside &lt;code class=&quot;highlighter-rouge&quot;&gt;NEWROOT&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that you have understood these three main concepts, let us create a container image from the following dockerfile, which we want to run.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04
COPY script.py /app
CMD python /app/script.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This container image contains the ubuntu:18.04 userland file structure heirarchy, /app/script.py and some environment configuration. Ignoring the config part for now, your minimal implementation can run this image in just 4 steps.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Export and extract contents of the image in &lt;code class=&quot;highlighter-rouge&quot;&gt;new_root_dir&lt;/code&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; mkdir new_root_dir
&amp;gt; docker export docker_image | tar -xf - -C new_root_dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Create a control group and set memory and CPU use limits
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; control_group=$(uuidgen)
&amp;gt; cgcreate -g cpu,memory:$control_group
&amp;gt; cgset -r memory.limit_in_bytes=50000000 $control_group
&amp;gt; cgset -r cpu.shares=256 $control_group
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Executing inside the control group, call unshare to separate namespaces and execute &lt;code class=&quot;highlighter-rouge&quot;&gt;script.py&lt;/code&gt; inside the &lt;code class=&quot;highlighter-rouge&quot;&gt;new_root_dir&lt;/code&gt; jail
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; cgexec -g cpu,memory:$control_group unshare -uinpUrf --mount-proc sh -c &quot;chroot new_root_dir /app/script.py&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Cleanup. Delete the cgroup and &lt;code class=&quot;highlighter-rouge&quot;&gt;new_root_dir&lt;/code&gt;. Unless bound to a file, namespaces cease to exist once all running processes in the namespace have exited.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; cgdelete -r -g cpu,memory:$control_group
&amp;gt; rm -r new_root_dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Lo and behold!&lt;/strong&gt; You have just created a minimal container runtime!&lt;/p&gt;

&lt;h1 id=&quot;whoa-wait-container-r-what&quot;&gt;Whoa! Wait, Container R… what?&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Container Runtime&lt;/strong&gt; - the code and tooling responsible for running containers. What you created above is the heart of what every container runtime does. Although, it catches the essence of container runtimes, it’s still minimal. Docker images also have something known as a &lt;code class=&quot;highlighter-rouge&quot;&gt;config.json&lt;/code&gt;. This file has, among other things, environment variables to be set for the running process inside the container, and the uid and gid of the user the process must run as.&lt;/p&gt;

&lt;p&gt;The code to run containers used to be deep inside a monolith called &lt;code class=&quot;highlighter-rouge&quot;&gt;Docker&lt;/code&gt;. But, it need not be. As long as vendors agree upon a common specification for images and a common specification for runtimes, &lt;em&gt;anybody could create runtimes&lt;/em&gt; customized to their needs. &lt;strong&gt;That’s exactly what they did&lt;/strong&gt;. Docker, CoreOS, Google and other industry leaders in the container space came together and launched &lt;a href=&quot;https://opencontainers.org&quot;&gt;Open Container Initiative&lt;/a&gt; in June 2015. OCI is responsible for defining &lt;a href=&quot;https://github.com/opencontainers/image-spec&quot;&gt;image-spec&lt;/a&gt; and &lt;a href=&quot;https://github.com/opencontainers/runtime-spec&quot;&gt;runtime-spec&lt;/a&gt;, which every OCI-compliant image builder and container runtime has to abide by.&lt;/p&gt;

&lt;p&gt;OCI even develops and maintains a reference implementation of the runtime-spec called &lt;a href=&quot;https://github.com/opencontainers/runc&quot;&gt;runc&lt;/a&gt;. runc broke off from Docker, as part of the Open Container Initiative. Although, runc is self-sufficient to run containers, it is a low-level runtime. The only developers that work with runc are developers of high-level runtimes.&lt;/p&gt;

&lt;h1 id=&quot;come-on-these-container-runtimes-have-levels-now&quot;&gt;Come on! These container runtimes have ‘levels’ now?&lt;/h1&gt;

&lt;p&gt;Yes, they very much do! If you ever used Docker, you might know that running containers from images isn’t all that you do. You might want to &lt;strong&gt;pull images&lt;/strong&gt; from registries before you actually run them. A higher level runtime does that for you.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/container_runtimes.png&quot; alt=&quot;&quot; title=&quot;A higher level runtime interacting with a lower level runtime&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Higher level runtimes are also responsible for &lt;strong&gt;unpacking&lt;/strong&gt; the container image into an &lt;a href=&quot;https://github.com/opencontainers/runtime-spec/blob/master/bundle.md&quot;&gt;OCI runtime bundle&lt;/a&gt; before spawning a runc process to run it. In addition to managing the lifecycle of a container, higher level runtimes are also sometimes responsible for low level storage and network namespace management. &lt;strong&gt;This is usually in place to facilitate interaction between individual container processes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Humans aren’t the only entities that interact with higher level runtimes. &lt;a href=&quot;https://www.redhat.com/en/topics/containers/what-is-container-orchestration&quot;&gt;Container orchestration&lt;/a&gt; services (&lt;em&gt;just a fancy term for management and configuration of containers across large dynamic systems&lt;/em&gt;), like &lt;a href=&quot;https://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt;, need to interact with high-level runtimes. For most industry use-cases, it’s less humans and more such services that talk to these runtimes.&lt;/p&gt;

&lt;h1 id=&quot;did-you-mention-kubernetes-you-had-my-curiosity-now-you-have-my-attention&quot;&gt;Did you mention Kubernetes? You had my curiosity. Now you have my attention.&lt;/h1&gt;

&lt;p&gt;What interacts with high-level container runtimes are not client-facing modules of a running Kubernetes instance, &lt;strong&gt;but a node-agent called&lt;/strong&gt; &lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/&quot;&gt;kubelet&lt;/a&gt; which runs on all nodes in a Kubernetes cluster. &lt;strong&gt;Kubelet&lt;/strong&gt; is responsible to ensure all containers mentioned in a pod’s specification are running and healthy. It registers nodes, sends pod status and events, and reports resource utilization higher up the command chain.&lt;/p&gt;

&lt;p&gt;With the introduction of OCI, many container runtimes came up that supported running OCI-compliant container images, and &lt;strong&gt;so arised the need for Kubernetes to support multiple runtimes&lt;/strong&gt;. To avoid deep integration of such runtimes into kubelet source code, and the subsequent maintenance that would follow, Kubernetes introduced the &lt;a href=&quot;https://github.com/kubernetes/cri-api&quot;&gt;Container Runtime Interface&lt;/a&gt; - an interface definition which enables kubelet to use a wide variety of runtimes. It is the responsibility of a container runtime to implement this interface as an internal package or as a &lt;strong&gt;shim&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/containerd&quot;&gt;containerd&lt;/a&gt;, a prominent high-level container runtime, which broke off from Docker similar to runc, recently merged its separate &lt;a href=&quot;https://github.com/containerd/cri&quot;&gt;cri-plugin&lt;/a&gt; codebase to its main &lt;a href=&quot;https://github.com/containerd/containerd&quot;&gt;containerd/containerd&lt;/a&gt; repository, &lt;strong&gt;marking CRI-implementation to be an important part of the container runtime&lt;/strong&gt;. &lt;a href=&quot;cri-o.io&quot;&gt;cri-o&lt;/a&gt; is another implementation of CRI, &lt;strong&gt;focused and optimized only for Kubernetes&lt;/strong&gt;, and, unlike containerd, can not service docker daemons for container orchestration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/crio_to_kernel.png&quot; alt=&quot;&quot; title=&quot;CRI-O to Kernel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that we have established CRI, let us talk about what the recent &lt;strong&gt;Kubernetes Docker Deprecation&lt;/strong&gt; really means.&lt;/p&gt;

&lt;h1 id=&quot;finally&quot;&gt;Finally!&lt;/h1&gt;

&lt;p&gt;Kuberenetes recently announced that it would be &lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation&quot;&gt;deprecating Docker&lt;/a&gt;. It really isn’t as dramatic as it sounds. What Kuberenetes will not support is &lt;strong&gt;Docker as a runtime&lt;/strong&gt;, and nothing else changes. Images built with dockerfiles are OCI-compliant and hence can be very well used with Kubernetes. Both containerd and cri-o know how to pull them, and runc knows how to run them.&lt;/p&gt;

&lt;p&gt;Docker, being built for human interaction, isn’t really friendly for Kubernetes as just a runtime. To interact with it, Kubernetes has to develop a module called &lt;strong&gt;dockershim&lt;/strong&gt;, which implements CRI support for Docker. This makes Docker call-able by kubelet as a runtime. Kubernetes is no longer willing to maintain this, especially when containerd (which Docker internally uses) has a CRI plugin. If you are developer, you do not really need to worry about what runtimes kubelet can interact with. &lt;strong&gt;Docker built images are perfectly fine for Kubernetes to consume!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/dockershim_containerd.png&quot; alt=&quot;&quot; title=&quot;Dockershim deprecation&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;end-notes&quot;&gt;End Notes&lt;/h1&gt;

&lt;p&gt;I hope you liked reading this blog post as much as I loved writing it. I’ll soon update a &lt;strong&gt;large&lt;/strong&gt; list of references which can be used for further reading. In the meanwhile, please feel free to follow me on &lt;a href=&quot;https://twitter.com/yash_jakhotiya&quot;&gt;Twitter&lt;/a&gt; and subscribe to the Blog’s &lt;a href=&quot;https://yashjakhotiya.github.io/blog/feed.xml&quot;&gt;RSS Feed&lt;/a&gt; for further updates. For any feedback or suggestions for blog posts, please drop an &lt;a href=&quot;mailto:mailsforyashj@gmail.com&quot;&gt;email&lt;/a&gt; or DM on Twitter. Thanks for reading!&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/yash-jakhotiya/'&gt;Yash Jakhotiya&lt;/a&gt;</name></author><summary type="html">Containers? Aren’t they … like some sort of virtual machines?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://yashjakhotiya.github.io/blog/images/%3Ctbd,%20creately.com%3E" /><media:content medium="image" url="https://yashjakhotiya.github.io/blog/images/%3Ctbd,%20creately.com%3E" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">GSoC ‘20: Kubeflow Customer User Journey Notebooks with Tensorflow 2.x Keras</title><link href="https://yashjakhotiya.github.io/blog/open-source/mlops/2020/08/23/gsoc-kubeflow.html" rel="alternate" type="text/html" title="GSoC '20: Kubeflow Customer User Journey Notebooks with Tensorflow 2.x Keras" /><published>2020-08-23T00:00:00-05:00</published><updated>2020-08-23T00:00:00-05:00</updated><id>https://yashjakhotiya.github.io/blog/open-source/mlops/2020/08/23/gsoc-kubeflow</id><content type="html" xml:base="https://yashjakhotiya.github.io/blog/open-source/mlops/2020/08/23/gsoc-kubeflow.html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Open source software development and &lt;a href=&quot;https://summerofcode.withgoogle.com/&quot;&gt;Google Summer of Code&lt;/a&gt;, both started long before the summer of 2020. When the world was starting to grapple with the realities of &lt;a href=&quot;https://www.entrepreneur.com/article/354872&quot;&gt;remote work&lt;/a&gt;, open source community was already thriving on it. Over the course of my college years, I have found out three things that I am passionate about - open source, machine learning and &lt;a href=&quot;https://landing.google.com/sre/&quot;&gt;SRE&lt;/a&gt;. &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow&lt;/a&gt; has managed to incorporate all of these into one and doing a project with this organisation has been a dream come true!&lt;/p&gt;

&lt;h1 id=&quot;goal&quot;&gt;Goal&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt; is already an industry-standard in managing cloud resources. &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow&lt;/a&gt; is on its path to become an industry standard in managing machine learning workflows on cloud. Examples that illustrate Kubeflow functionalities using latest industry technologies make Kubeflow easier to use and more accessible to all potential users. This project has aimed at building samples for Jupyter notebook to Kubeflow deployment using Tensorflow 2.0 Keras for backend training code, illustrating customer user journey (CUJ) in the process. This project has also served as an hands-on to large scale application of machine learning bringing in the elements of DevOps and SRE and this has kept me motivated throughout the project.&lt;/p&gt;

&lt;h1 id=&quot;the-kubeflow-community&quot;&gt;The Kubeflow Community&lt;/h1&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt;Kubeflow community&lt;/a&gt; is a highly approachable and closely-knit community that has been reaching out to and &lt;a href=&quot;https://www.kubeflow.org/docs/about/gsoc/&quot;&gt;helping&lt;/a&gt; potential GSoC students well before the application period. Respecting this, I made sure to take feedback for my proposal of the &lt;a href=&quot;https://summerofcode.withgoogle.com/projects/#5507335985823744&quot;&gt;project idea&lt;/a&gt; I chose, before the application deadline. Mentors &lt;a href=&quot;https://github.com/terrytangyuan&quot;&gt;Yuan Tang&lt;/a&gt;, &lt;a href=&quot;https://github.com/gaocegege&quot;&gt;Ce Gao&lt;/a&gt; and &lt;a href=&quot;https://github.com/ChanYiLin&quot;&gt;Jack Lin&lt;/a&gt; were candid in providing me feedback and I refined and changed my proposal accordingly. To my sweet surprise, I got selected for the idea!😁 What has really helped me in these three months of coding period is that one month of &lt;a href=&quot;https://developers.google.com/open-source/gsoc/timeline&quot;&gt;community bonding&lt;/a&gt; where I got to know the community and more about the technicalities of Kubeflow.&lt;/p&gt;

&lt;h1 id=&quot;the-project&quot;&gt;The Project&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-08-23-gsoc-20-tf-2-examples/kubeflow_components.png&quot; alt=&quot;&quot; title=&quot;Kubeflow Components&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Examples created as part of this project needed to be easily reproducible to serve their purpose. Initially the underlying model decided to demonstrate Kubeflow functionalities was a BiDirectional RNN to be trained on IMDB large movie review &lt;a href=&quot;http://ai.stanford.edu/%7Eamaas/data/sentiment/&quot;&gt;dataset&lt;/a&gt; for sentiment analysis based on a &lt;a href=&quot;https://www.tensorflow.org/tutorials/text/text_classification_rnn&quot;&gt;tensorflow tutorial&lt;/a&gt;. Over the course of time, we decided to also add another set of examples using  a neural machine translation model in its backend trained on a Spanish to English &lt;a href=&quot;http://www.manythings.org/anki/&quot;&gt;dataset&lt;/a&gt; based on another &lt;a href=&quot;https://www.tensorflow.org/tutorials/text/nmt_with_attention&quot;&gt;tensorflow tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The reasons for choosing these models were:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/kubeflow/examples&quot;&gt;kubeflow/examples&lt;/a&gt; repo needed more NLP-related tasks.&lt;/li&gt;
  &lt;li&gt;These were more of &lt;em&gt;hello world&lt;/em&gt; tasks in the field of NLP. So that users who go through these samples need not worry about training code and focus more on Kubeflow’s functionalities.&lt;/li&gt;
  &lt;li&gt;These are based on tensorflow tutorials. Kubeflow tutorials based on Tensorflow tutorials show better coupling between the two.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;repository-structure&quot;&gt;Repository Structure&lt;/h2&gt;

&lt;p&gt;I created a &lt;a href=&quot;https://github.com/yashjakhotiya/kubeflow-gsoc-2020&quot;&gt;repo&lt;/a&gt; under my own profile to regularly push commits to and my mentors consistently reviewed the work I pushed there. This repo has all of my work with the log history preserved. Each of the two models has the following folder structure explaining core Kubeflow functionalities -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;training-model&amp;gt;.py&lt;/code&gt; - This is the core training code upon which all subsequent examples showing Kubeflow functionalities are based. Please go through this first to know more about the machine learning task subsequent notebooks will manage. For example check the &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/text_classification_rnn.py&quot;&gt;source code&lt;/a&gt; of the model used for the text classification task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;distributed_&amp;lt;training-model&amp;gt;.py&lt;/code&gt; - To truly take advantage of multiple compute nodes, the training code has to be modified to support distributed training. The code in the above mentioned file is modified with Tensorflow’s &lt;a href=&quot;https://www.tensorflow.org/guide/distributed_training&quot;&gt;distributed training&lt;/a&gt; strategy and hosted &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/distributed_text_classification_rnn.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; - This is the dockerfile which is used to build Docker image of the training code. Some Kubeflow functionalities require that a docker image of the training code is built and hosted on a docker container registry. This Docker 101 &lt;a href=&quot;https://www.docker.com/101-tutorial&quot;&gt;tutorial&lt;/a&gt; is a good starting point to get hands-on training on Docker. For complete starters in the field of containerization, this &lt;a href=&quot;https://opensource.com/resources/what-docker&quot;&gt;introduction&lt;/a&gt; can serve as a good starting point. The dockerfile used with the source code mentioned above can be found &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/Dockerfile&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fairing-with-python-sdk.ipynb&lt;/code&gt; - Fairing is a Kubeflow functionality that lets you run model training tasks remotely. This is the Jupyter notebook which deploys a model training task on cloud using Kubeflow Fairing. Fairing does not require you to build a Docker image of the training code first. Hence, its training code resides in the same notebook. To know more about Kubeflow Fairing, please visit Fairing’s &lt;a href=&quot;https://www.kubeflow.org/docs/components/fairing/fairing-overview/&quot;&gt;official documentation&lt;/a&gt;. To get a better idea about Fairing, you can take a look at the text classification Fairing notebook &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/fairing-with-python-sdk.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;katib-with-python-sdk.ipynb&lt;/code&gt; - &lt;a href=&quot;https://www.kubeflow.org/docs/components/hyperparameter-tuning/hyperparameter/&quot;&gt;Katib&lt;/a&gt; is a Kubeflow functionality that lets you perform hyperparameter tuning experiments and reports best set of hyperparameters based on a provided metric. This is the Jupyter notebook which launches Katib hyperparameter tuning experiments using its &lt;a href=&quot;https://github.com/kubeflow/katib/tree/master/sdk/python/v1alpha3&quot;&gt;Python SDK&lt;/a&gt;. Katib requires you to build and host a Docker image of your training code in a container registry. For this sample, we have used &lt;a href=&quot;https://cloud.google.com/cloud-build/docs&quot;&gt;gcloud builds&lt;/a&gt; to build the required Docker image of the training code along with the training data and host it on &lt;a href=&quot;https://cloud.google.com/container-registry&quot;&gt;Google Container Registry&lt;/a&gt;. This is the &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/katib-with-python-sdk.ipynb&quot;&gt;notebook&lt;/a&gt; we used to demonstrate Katib for the text classification task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tfjob-with-python-sdk.ipynb&lt;/code&gt; - &lt;a href=&quot;https://www.kubeflow.org/docs/components/training/tftraining/&quot;&gt;TFJobs&lt;/a&gt; are used to run distributed training jobs over Kubernetes. With multiple workers, TFJob truly leverage the ability of your code to support distributed training. This Jupyter notebook demonstrates how to use TFJob. The Docker image built from the distributed version of our core training code is used in this notebook. TFJob notebook for the text classification task can be found &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/tfjob-with-python-sdk.ipynb&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tekton-pipeline-with-python-sdk.ipynb&lt;/code&gt; - &lt;a href=&quot;https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/&quot;&gt;Kubeflow Pipeline&lt;/a&gt; is a platform that lets you build, manage and deploy end-to-end machine learning workflows. This is a Jupyter notebook which bundles Katib hyperparameter tuning and TFJob distributed training into one Kubeflow pipeline. The pipeline used here uses &lt;a href=&quot;https://cloud.google.com/tekton&quot;&gt;Tekton&lt;/a&gt; in its backend. Tekton is a Kubernetes resource to create efficient &lt;a href=&quot;https://opensource.com/article/18/8/what-cicd&quot;&gt;continuous integration and delivery&lt;/a&gt; (CI/CD) systems. The pipeline notebook for the text classification task can be found at &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/tekton-pipeline-with-python-sdk.ipynb&quot;&gt;this&lt;/a&gt; place.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;merged-prs&quot;&gt;Merged PRs&lt;/h2&gt;

&lt;p&gt;In the community bonding period, I opened numerous small issues and PRs solving these issues, as I encountered them when reading documentation or implementing an example. This was done in an effort to know more about the Kubeflow community.&lt;/p&gt;

&lt;p&gt;For the main project, I copied these built notebooks and the final work product into a directory created in my fork of the &lt;a href=&quot;https://github.com/kubeflow/examples&quot;&gt;kubeflow/examples&lt;/a&gt; repo and created a &lt;a href=&quot;https://github.com/kubeflow/examples/pull/816&quot;&gt;PR&lt;/a&gt; to add these notebooks in Kubeflow’s official repo. The PR got merged and the code currently resides in &lt;a href=&quot;https://github.com/kubeflow/examples/tree/master/tensorflow_cuj&quot;&gt;kubeflow/examples/tensorflow_cuj&lt;/a&gt; directory marking the completion of the project.&lt;/p&gt;

&lt;h1 id=&quot;special-thanks&quot;&gt;Special Thanks&lt;/h1&gt;

&lt;p&gt;Special thanks are due to -&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;My mentors for their valuable guidance throughout the project.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/jeremy-lewi-600aaa8/&quot;&gt;Jeremy&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/sarahmaddox/&quot;&gt;Sarah&lt;/a&gt; for smooth conduction of the Kubeflow GSoC program.&lt;/li&gt;
  &lt;li&gt;The GSoC Discord &lt;a href=&quot;https://discord.com/channels/708636399666069514/708636400097951744&quot;&gt;Server&lt;/a&gt; and the GSoC Telegram &lt;a href=&quot;https://web.telegram.org/#/im?p=s1263176603_5411849872541551939&quot;&gt;Channel&lt;/a&gt; for the help, casual talks and a strong global student community.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/yash-jakhotiya/'&gt;Yash Jakhotiya&lt;/a&gt;</name></author><summary type="html">Introduction</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://yashjakhotiya.github.io/blog/images/2020-08-23-gsoc-20-tf-2-examples/logos.jpg" /><media:content medium="image" url="https://yashjakhotiya.github.io/blog/images/2020-08-23-gsoc-20-tf-2-examples/logos.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>