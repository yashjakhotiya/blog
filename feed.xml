<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://yashjakhotiya.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yashjakhotiya.github.io/blog/" rel="alternate" type="text/html" /><updated>2021-12-06T05:36:27-06:00</updated><id>https://yashjakhotiya.github.io/blog/feed.xml</id><title type="html">Blog</title><subtitle>Blog</subtitle><entry><title type="html">pinv »&amp;gt; inv</title><link href="https://yashjakhotiya.github.io/blog/linear%20algebra/2021/11/07/moore-penrose-inversion.html" rel="alternate" type="text/html" title="pinv &gt;&gt;&gt; inv" /><published>2021-11-07T00:00:00-05:00</published><updated>2021-11-07T00:00:00-05:00</updated><id>https://yashjakhotiya.github.io/blog/linear%20algebra/2021/11/07/moore-penrose-inversion</id><content type="html" xml:base="https://yashjakhotiya.github.io/blog/linear%20algebra/2021/11/07/moore-penrose-inversion.html">&lt;p&gt;In this post, let’s go all the way back to something as “basic” as linear regression and it’s closed form solution through least squares method and try to unearth a couple of linear algebra gems.&lt;/p&gt;

&lt;h1 id=&quot;when-explicit-formulation-failed&quot;&gt;When explicit formulation failed&lt;/h1&gt;
&lt;p&gt;In an ML class, we were told to find out weights to a linear regression problem through least squares method, before we went to gradient descent. My first instinct was to use the formulation (derivation in the &lt;a href=&quot;https://mahdi-roozbahani.github.io/CS46417641-fall2021/course/15-linear-regression-note.pdf&quot;&gt;slides&lt;/a&gt;) -&lt;/p&gt;

&lt;span class=&quot;katex-display&quot;&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\theta = (X^TX)^{-1}X^TY&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.1413309999999999em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.864108em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;−&lt;/span&gt;&lt;span class=&quot;mord mtight&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.22222em;&quot;&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;p&gt;But I didn’t pass our autograder tests.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Because -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The above formulation has an assumption that $X^TX$ has to be invertible.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This will happen if rank of $X^TX$, a DxD matrix, is equal to D.&lt;/p&gt;

&lt;p&gt;$rank(X^TX)$ is always equal to  $rank(X)$. So we effectively need  $rank(X)$ to be D.  $X$ has rank  $D$ when $N &amp;gt;= D$ and  $X$ has linearly independent columns (i.e. linear independence in the  $D$ dimension).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BUT&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The autograder tests don’t have such assumptions. For example, if $N &amp;lt; D$ then $rank(X^TX) &amp;lt;= N &amp;lt; D$ OR if $N &amp;gt;= D$ and the columns (i.e. $X$’s features) are not linearly independent, which makes  $rank(X)$ again less than D, in which case we were told to use &lt;code class=&quot;highlighter-rouge&quot;&gt;pinv&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;But why &lt;code class=&quot;highlighter-rouge&quot;&gt;pinv&lt;/code&gt;? What is so special about it?&lt;/em&gt; Read on to find out!&lt;/p&gt;

&lt;h1 id=&quot;minimum-norm-solution---with-derivatives&quot;&gt;Minimum norm solution - with derivatives&lt;/h1&gt;
&lt;p&gt;Our aim in a linear regression problem to form a minimum norm solution, i.e. a solution which will have our loss (square of the norm, square of the L2 distance between predicted and true values) minimum. One approach to do this is by taking a derivative of the norm and equating to zero, which we did in class (slides &lt;a href=&quot;https://mahdi-roozbahani.github.io/CS46417641-fall2021/course/15-linear-regression-note.pdf&quot;&gt;here&lt;/a&gt;!), and which leads to the formulation -&lt;/p&gt;

&lt;span class=&quot;katex-display&quot;&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;X^TX\theta = X^TY&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8913309999999999em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8913309999999999em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.22222em;&quot;&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;p&gt;But the problem here is $X^TX$ can be invertible or can not be invertible, as stated above in the first section.&lt;/p&gt;

&lt;h1 id=&quot;better-minimum-norm-solution---with-svd&quot;&gt;(Better) minimum norm solution - with SVD&lt;/h1&gt;
&lt;p&gt;Another approach to find a minimum norm solution is by first finding singular value decomposition of  $X$&lt;/p&gt;

&lt;span class=&quot;katex-display&quot;&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Σ&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;X = UΣV^T&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.68333em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8913309999999999em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.10903em;&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;Σ&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.22222em;&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;p&gt;and defining $X^+$ to be&lt;/p&gt;

&lt;span class=&quot;katex-display&quot;&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Σ&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;X^+=VΣ^+U^T&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.821331em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.821331em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mbin mtight&quot;&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8913309999999999em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.22222em;&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;Σ&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.821331em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mbin mtight&quot;&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.10903em;&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;p&gt;where $Σ^+$ is obtained by taking the reciprocal of each non-zero element on the diagonal, leaving the zeros in place, and then transposing the matrix.&lt;/p&gt;

&lt;p&gt;and finding theta by&lt;/p&gt;

&lt;span class=&quot;katex-display&quot;&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;/msup&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Σ&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\theta = X^+Y = VΣ^+U^TY&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.821331em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.821331em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mbin mtight&quot;&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.22222em;&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8913309999999999em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.22222em;&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;Σ&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.821331em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mbin mtight&quot;&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.10903em;&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.22222em;&quot;&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;p&gt;The proof on why this theta gives the minimum norm solution is given &lt;a href=&quot;http://web.cs.ucla.edu/~chohsieh/teaching/CS260_Winter2019/notes_linearregression.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;enter-moore-penrose&quot;&gt;Enter Moore-Penrose&lt;/h1&gt;
&lt;p&gt;The NumPy &lt;code class=&quot;highlighter-rouge&quot;&gt;pinv&lt;/code&gt; we were using to solve the assignment was NOT&lt;/p&gt;

&lt;span class=&quot;katex-display&quot;&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;(X^TX)^{-1}X^T&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.1413309999999999em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.864108em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;−&lt;/span&gt;&lt;span class=&quot;mord mtight&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;p&gt;but the SVD $X^+$ defined above, also known as the Moore-Penrose inversion or ‘pseudoinverse’. Don’t believe me? Look for yourself &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.linalg.`pinv`.html&quot;&gt;here&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;This is why our autograder tests passed when using &lt;code class=&quot;highlighter-rouge&quot;&gt;pinv&lt;/code&gt;, because we were giving a minimum norm solution without relying on $X^TX$ invertibility.&lt;/p&gt;

&lt;h1 id=&quot;relation-between-the-two-approaches&quot;&gt;Relation between the two approaches&lt;/h1&gt;
&lt;p&gt;But how does this Moore-Penrose inversion, the SVD based X^+ definition, the NumPy &lt;code class=&quot;highlighter-rouge&quot;&gt;pinv&lt;/code&gt; relate to what we proved in class? Well, it reduces to&lt;/p&gt;

&lt;span class=&quot;katex-display&quot;&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;(X^TX)^{-1}X^T&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.1413309999999999em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.864108em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;−&lt;/span&gt;&lt;span class=&quot;mord mtight&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8913309999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;p&gt;when $X^TX$ is invertible!&lt;/p&gt;

&lt;p&gt;Read more about Moore-Penrose inversion &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;here&lt;/a&gt;. The article defines Moore-Penrose inversion based on 4 Moore-Penrose conditions and then arrives at SVD based computation, but both of these are equivalent. You can define by SVD based computation and then prove those 4 conditions. Also, don’t worry about the A* notation there. It is known as Hermitian transpose, and is equal to the ‘vanilla’ transpose for real numbers.&lt;/p&gt;

&lt;p&gt;Please feel free to correct me if you feel there’s a mistake anywhere.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/yash-jakhotiya/'&gt;Yash Jakhotiya&lt;/a&gt;</name></author><summary type="html">In this post, let’s go all the way back to something as “basic” as linear regression and it’s closed form solution through least squares method and try to unearth a couple of linear algebra gems.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://yashjakhotiya.github.io/blog/images/moore-penrose.png" /><media:content medium="image" url="https://yashjakhotiya.github.io/blog/images/moore-penrose.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">On Continual Learning</title><link href="https://yashjakhotiya.github.io/blog/continual%20learning/lifelong%20learning/2021/10/23/continual-learning.html" rel="alternate" type="text/html" title="On Continual Learning" /><published>2021-10-23T00:00:00-05:00</published><updated>2021-10-23T00:00:00-05:00</updated><id>https://yashjakhotiya.github.io/blog/continual%20learning/lifelong%20learning/2021/10/23/continual-learning</id><content type="html" xml:base="https://yashjakhotiya.github.io/blog/continual%20learning/lifelong%20learning/2021/10/23/continual-learning.html">&lt;h1 id=&quot;why-care-the-problem-and-my-personal-encounter-with-it&quot;&gt;Why care? The problem and my personal encounter with it&lt;/h1&gt;
&lt;p&gt;In a past internship in a large company’s private cloud department, I was tasked to train a model that would predict which on-call engineer an infrastuctural alert should go to, with features that ranged from time of the day to the alert’s natural language description. Fair enough. There goes two months of feature engineering, adapting and evaluating state-of-the-art models, hyperparameter tuning, finalising a two-stage network and deployment, and we have a module that performs acceptably well. All stakeholders are happy. Pushed to prod!&lt;/p&gt;

&lt;p&gt;Does the story end there? Nope. You see, a large dynamic private cloud has new alerts added every week. Can we feed in the new alerts and expect our module to spit something out? Yes. Can we assume that our on-call engineer roles, our classes, will always be the same? Probably not. Can we assume these new alerts are from an identical distribution as the old ones? Definitely not. (You’d be surprised at the number of new and innovative problems that arise in a complex cloud environment!)&lt;/p&gt;

&lt;p&gt;How do you make sure in this case, that your network learns to correctly classify these new alerts? A naive strategy would be to retrain the model on the whole (old + new) alerts data. This might work if -
i. the network training time is bearable
ii. the addition of new data doesn’t quickly result in a storage problem, especially if new data is available as a regular stream
iii. you’ll always have access to old data without any legal or privacy concerns.&lt;/p&gt;

&lt;p&gt;However, more of than you’d expect, one of the above assumptions gets violated and you are forced to think how to go about including newer data in your model. Another naive strategy would be to fine-tune the old model on newer data, and expect it to perform well on everything. In this case, you are in for another suprise - of how quickly your model learns to forget older data. Eventually, this leads us to …&lt;/p&gt;

&lt;h1 id=&quot;the-problem-of-catastrophic-forgetting&quot;&gt;The problem of catastrophic forgetting&lt;/h1&gt;
&lt;p&gt;As early as in 1989, McCloskey and Cohen&lt;a href=&quot;McCloskey, M. and Cohen, N. J. (1989). Catastrophic interference in connectionist networks: The&quot;&gt;3&lt;/a&gt; identified a problem - when trained sequentially on new data, classes and tasks, a neural network’s performance degrades at previously learned concepts. They named this ‘catastrophic interference’, now commonly known as catastrophic forgetting. This usually happens when a new task overrides previously learned weights, interfering with performance on past tasks. Abraham and Robins&lt;a href=&quot;Abraham WC, Robins A. Memory retention--the synaptic stability versus plasticity dilemma. Trends Neurosci. 2005;28(2):73-78. doi:10.1016/j.tins.2004.12.003&quot;&gt;4&lt;/a&gt; referred to this as the ‘stability-plasticity dilemma’, where a model too &lt;em&gt;stable&lt;/em&gt; won’t be able to learn new tasks and a model too &lt;em&gt;plastic&lt;/em&gt; will have it’s weights too changed to forget older ones. The research that goes into addressing catastrophic forgetting&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;author    = {Timoth{'{e}}e Lesort},
  title     = {Continual Learning: Tackling Catastrophic Forgetting in Deep Neural
               Networks with Replay Processes},
  journal   = {CoRR},
  volume    = {abs/2007.00487},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.00487},
  eprinttype = {arXiv},
  eprint    = {2007.00487},
  timestamp = {Mon, 06 Jul 2020 15:26:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-00487.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}&lt;/p&gt;

&lt;p&gt;author = {Chen, Zhiyuan and Liu, Bing and Brachman, Ronald and Stone, Peter and Rossi, Francesca},
title = {Lifelong Machine Learning},
year = {2018},
isbn = {1681733021},
publisher = {Morgan &amp;amp; Claypool Publishers},
edition = {2nd},
abstract = {Lifelong Machine Learning, Second Edition is an introduction to an advanced machine
learning paradigm that continuously learns by accumulating past knowledge that it
then uses in future learning and problem solving. In contrast, the current dominant
machine learning paradigm learns in isolation: given a training dataset, it runs a
machine learning algorithm on the dataset to produce a model that is then used in
its intended application. It makes no attempt to retain the learned knowledge and
use it in subsequent learning. Unlike this isolated system, humans learn effectively
with only a few examples precisely because our learning is very knowledge-driven:
the knowledge learned in the past helps us learn new things with little data or effort.
Lifelong learning aims to emulate this capability, because without it, an AI system
cannot be considered truly intelligent. Research in lifelong learning has developed
significantly in the relatively short time since the first edition of this book was
published. The purpose of this second edition is to expand the definition of lifelong
learning, update the content of several chapters, and add a new chapter about continual
learning in deep neural networkswhich has been actively researched over the past two
or three years. A few chapters have also been reorganized to make each of them more
coherent for the reader. Moreover, the authors want to propose a unified framework
for the research area. Currently, there are several research topics in machine learning
that are closely related to lifelong learningmost notably, multi-task learning, transfer
learning, and meta-learningbecause they also employ the idea of knowledge sharing
and transfer. This book brings all these topics under one roof and discusses their
similarities and differences. Its goal is to introduce this emerging machine learning
paradigm and present a comprehensive survey and review of the important research results
and latest ideas in the area. This book is thus suitable for students, researchers,
and practitioners who are interested in machine learning, data mining, natural language
processing, or pattern recognition. Lecturers can readily use the book for courses
in any of these related fields.}
}&lt;/p&gt;

&lt;p&gt;sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109–165.
Elsevier.&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/yash-jakhotiya/'&gt;Yash Jakhotiya&lt;/a&gt;</name></author><summary type="html">Why care? The problem and my personal encounter with it In a past internship in a large company’s private cloud department, I was tasked to train a model that would predict which on-call engineer an infrastuctural alert should go to, with features that ranged from time of the day to the alert’s natural language description. Fair enough. There goes two months of feature engineering, adapting and evaluating state-of-the-art models, hyperparameter tuning, finalising a two-stage network and deployment, and we have a module that performs acceptably well. All stakeholders are happy. Pushed to prod!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/runc_in_docker.png" /><media:content medium="image" url="https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/runc_in_docker.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Can you, like, REALLY explain me this?</title><link href="https://yashjakhotiya.github.io/blog/resources/2021/10/11/list-of-resources.html" rel="alternate" type="text/html" title="Can you, like, REALLY explain me this?" /><published>2021-10-11T00:00:00-05:00</published><updated>2021-10-11T00:00:00-05:00</updated><id>https://yashjakhotiya.github.io/blog/resources/2021/10/11/list-of-resources</id><content type="html" xml:base="https://yashjakhotiya.github.io/blog/resources/2021/10/11/list-of-resources.html">&lt;p&gt;There are times when you need to look back to something and reassure yourself that you &lt;em&gt;really&lt;/em&gt; &lt;em&gt;really&lt;/em&gt; understand it. This is a dynamic work-in-progress list of resources that has, potentially in a distant past, helped me do that.&lt;/p&gt;

&lt;h1 id=&quot;linear-algebra&quot;&gt;Linear Algebra&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=P2LTAUO1TdA&quot;&gt;Change of basis&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PFDu9oVAE-g&quot;&gt;Eigenvectors and Eigenvalues&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://yashjakhotiya.github.io/blog/linear%20algebra/2021/11/07/moore-penrose-inversion.html&quot;&gt;Matrix Ranks, Invertibility, SVD, Pseudoinverse&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;statistical-ml&quot;&gt;Statistical ML&lt;/h1&gt;

&lt;h2 id=&quot;probability-mle-em&quot;&gt;Probability, MLE, EM&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://courses.engr.illinois.edu/cs440/sp2011/slides/lecture15.pdf&quot;&gt;Multiple random variables probability review&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://mahdi-roozbahani.github.io/CS46417641-fall2021/course/09-gaussian-mixture-note.pdf&quot;&gt;Generative modeling, Latent variables, GMMs, MLE, EM&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;naive-bayes-logistic---sigmoid-softmax&quot;&gt;Naive Bayes, Logistic - Sigmoid, Softmax&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://cocoxu.github.io/CS7650_fall2021/slides/lec2-ml.pdf&quot;&gt;MLE, Naive Bayes, Logistic&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;www.youtube.com/watch?v=8CWyBNX6eDo&quot;&gt;Logistic, Softmax, Cross-entropy, Logistic connection to a single neuron via sigmoid&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;neural-network-foundational&quot;&gt;Neural Network Foundational&lt;/h1&gt;

&lt;h2 id=&quot;space-transformation&quot;&gt;Space Transformation&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/&quot;&gt;Neural Networks, Manifolds, and Topology&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Ilg3gGewQ5U&quot;&gt;What is backpropagation really doing?&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=tIeHLnjs5U8&quot;&gt;Backpropagation calculus&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=yLYHDSv-288&quot;&gt;Backprop with practical considerations&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;natural-language-processing&quot;&gt;Natural Language Processing&lt;/h1&gt;

&lt;h2 id=&quot;non-contextual-word-embeddings&quot;&gt;Non Contextual Word Embeddings&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://cocoxu.github.io/CS7650_fall2021/slides/lec7-nn2.pdf&quot;&gt;Word2Vec CBOW and Skip-Gram&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://paperswithcode.com/method/glove&quot;&gt;GloVe training objective&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;attention&quot;&gt;Attention&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://cocoxu.github.io/CS7650_fall2021/slides/lec11-seq2seq.pdf&quot;&gt;Seq2Seq problems - need for Attention, Attention, NMT, BPE, WordPieces&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/yash-jakhotiya/'&gt;Yash Jakhotiya&lt;/a&gt;</name></author><summary type="html">There are times when you need to look back to something and reassure yourself that you really really understand it. This is a dynamic work-in-progress list of resources that has, potentially in a distant past, helped me do that.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://yashjakhotiya.github.io/blog/images/list-of-resources/linked-tori.png" /><media:content medium="image" url="https://yashjakhotiya.github.io/blog/images/list-of-resources/linked-tori.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Can we avoid repeated questions by identifying similar intent?</title><link href="https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html" rel="alternate" type="text/html" title="Can we avoid repeated questions by identifying similar intent?" /><published>2021-10-03T00:00:00-05:00</published><updated>2021-10-03T00:00:00-05:00</updated><id>https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs</id><content type="html" xml:base="https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html">&lt;p&gt;&lt;em&gt;Course project for &lt;a href=&quot;https://mahdi-roozbahani.github.io/CS46417641-fall2021/&quot;&gt;CS 7641&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;introductionbackground&quot;&gt;Introduction/Background&lt;/h1&gt;

&lt;p&gt;With public discussion forums becoming highly popular in recent times, especially with classes and work moving to an online setup, websites like Edstem, Piazza, Stack Overflow and Quora have a constant influx of a large volume of questions. Multiple questions with the same intent leads to redundancy, and waste of time, space and resources.&lt;/p&gt;

&lt;h1 id=&quot;problem-definition&quot;&gt;Problem Definition&lt;/h1&gt;

&lt;p&gt;The aim is to identify and flag questions with a high similarity index, and retain only canonical questions in order to make the work of administrative staff such as TAs/Professors easier in terms of answering repeated questions or questions of the same intent.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;Dataset: The dataset that we will be using is the Quora Question Pairs dataset&lt;sup&gt;[1]&lt;/sup&gt;. It consists of 404,290 pairs of questions. Each datapoint consists of a pair of questions and whether or not they are similar.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to gain useful insights, we visualize and understand patterns in our data using the following plots.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/22400185/141877000-0663d2d9-2a99-4f59-b7ee-1cd367323457.png&quot; alt=&quot;WordCloud&quot; /&gt;
&lt;em&gt;Fig 1: Word Cloud depicting which words appear more frequently in the dataset&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/22400185/141876903-478cd635-ae3a-45c8-a491-7f71b0c14011.jpeg&quot; alt=&quot;Histogram of character count&quot; /&gt;
&lt;em&gt;Fig 2: Histogram representing the number of characters in each question&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/22400185/141876911-18ff1378-cb7a-411c-b82b-f13c571eddd2.jpeg&quot; alt=&quot;Histogram of word count&quot; /&gt;
&lt;em&gt;Fig 3: Histogram representing the number of words in each question&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/22400185/141876917-4d315806-63bb-4a46-a7a4-c55c6acff8ac.jpeg&quot; alt=&quot;Word share&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 4: Plots representing the distribution of ratio of words shared between similar and dissimlar question&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data Augmentation:
    &lt;ul&gt;
      &lt;li&gt;We exploit the transitive property of similarity to generate new datapoints.
        &lt;ul&gt;
          &lt;li&gt;If Question Q&lt;sub&gt;1&lt;/sub&gt; is similar to Question Q&lt;sub&gt;2&lt;/sub&gt; and Question Q&lt;sub&gt;2&lt;/sub&gt; is similar to Question Q&lt;sub&gt;3&lt;/sub&gt;, then we can infer that Q&lt;sub&gt;1&lt;/sub&gt; is similar to Q&lt;sub&gt;3&lt;/sub&gt;&lt;/li&gt;
          &lt;li&gt;If Question Q&lt;sub&gt;1&lt;/sub&gt; is similar to Question Q&lt;sub&gt;2&lt;/sub&gt; and Question Q&lt;sub&gt;2&lt;/sub&gt; is not similar to Question Q&lt;sub&gt;3&lt;/sub&gt;, then we can infer that Q&lt;sub&gt;1&lt;/sub&gt; is not similar to Q&lt;sub&gt;3&lt;/sub&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Through this method, we generated 25% more training datapoints.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Data preprocessing:
    &lt;ul&gt;
      &lt;li&gt;We used the following preprocessing techniques:
        &lt;ul&gt;
          &lt;li&gt;Sample Sentence: “How can internet speed be increased by hacking through DNS?”
            &lt;ol&gt;
              &lt;li&gt;Removing punctuation&lt;/li&gt;
            &lt;/ol&gt;
            &lt;ul&gt;
              &lt;li&gt;After Punctuation Removal: “How can internet speed be increased by hacking through DNS”
          2. Converting characters to Lower Case&lt;/li&gt;
              &lt;li&gt;After conversion: “how can internet speed be increased by hacking through dns”
          3. Tokenization&lt;/li&gt;
              &lt;li&gt;Tokenization is the process of converting the sentence into a list of it’s constituent words/phrases. It helps reduce the input to a finite set, making it easier to process.&lt;/li&gt;
              &lt;li&gt;After Tokenization: [‘how’, ‘can’, ‘internet’, ‘speed’, ‘be’, ‘increased’, ‘by’, ‘hacking’, ‘through’, ‘dns’]
          4. Stop Word Removal&lt;/li&gt;
              &lt;li&gt;The idea of Stop Word Removal is to remove commonly occuring words in the corpus.&lt;/li&gt;
              &lt;li&gt;After Stop Word Removal: [‘internet’, ‘speed’, ‘increased’, ‘hacking’, ‘dns’]
          5. POS Tagging&lt;/li&gt;
              &lt;li&gt;POS tagging refers to categorizing words in a corpus corresponding to a particular part of speech (eg: Noun, Verb, Adjective etc). This process helps improve the performance of the Lemmatizer.&lt;/li&gt;
              &lt;li&gt;After POS Tagging: [(‘internet’, ‘a’), (‘speed’, ‘n’), (‘increased’, ‘v’), (‘hacking’, ‘v’), (‘dns’, ‘n’)]
          6. Lemmatization&lt;/li&gt;
              &lt;li&gt;Lemmatization is grouping together different inflected forms of a word so that they can be analyzed as a single item.&lt;/li&gt;
              &lt;li&gt;After Lemmatization: [‘internet’, ‘speed’, ‘increase’, ‘hack’, ‘dns’]&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Training:
    &lt;ul&gt;
      &lt;li&gt;For each training iteration, we input questions in a pairwise fashion - 𝑄&lt;sub&gt;𝑖&lt;/sub&gt;, 𝑄&lt;sub&gt;𝑗&lt;/sub&gt;.&lt;/li&gt;
      &lt;li&gt;The model learns representations of building blocks of both sentences𝑄&lt;sub&gt;𝑖&lt;/sub&gt;−&amp;gt;ν&lt;sub&gt;𝑖&lt;/sub&gt;, 𝑄&lt;sub&gt;𝑗&lt;/sub&gt;−&amp;gt;ν&lt;sub&gt;𝑗&lt;/sub&gt;.&lt;/li&gt;
      &lt;li&gt;Next, these representations are concatenated 𝑣&lt;sub&gt;𝑐𝑜𝑛&lt;/sub&gt; = 𝑐𝑜𝑛𝑐𝑎𝑡(ν&lt;sub&gt;𝑖&lt;/sub&gt;, ν&lt;sub&gt;𝑗&lt;/sub&gt;), and passed on to a feedforward neural network or a machine learning model 𝐹 (𝑣𝑐𝑜𝑛) that predicts whether two questions are similar or not.&lt;/li&gt;
      &lt;li&gt;∀ questions 𝑄&lt;sub&gt;𝑖&lt;/sub&gt; ε question bank 𝑄&lt;sub&gt;𝐵&lt;/sub&gt;, we group them into clusters 𝑐&lt;sub&gt;&lt;em&gt;1&lt;/em&gt;&lt;/sub&gt;,…,𝑐&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt; for efficient inference.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;strong&gt;Supervised method: Classifying sentences as similar or not using  BERT-based model (Added for midpoint report)&lt;/strong&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;The tokenized version of the candidate sentence that we get from the preprocessing stage is first passed to a pre-trained BERT model.&lt;/li&gt;
      &lt;li&gt;The output of this pre-trained BERT model is the sentence-level embedding of the tokenized version - E&lt;sub&gt;1.&lt;/sub&gt;&lt;/li&gt;
      &lt;li&gt;The same process is done on the second candidate sentence to retrieve its sentence-level embedding - E&lt;sub&gt;2.&lt;/sub&gt;&lt;/li&gt;
      &lt;li&gt;The two embeddings are concatenated, and then passed to a feed-forward neural network which consists of a fully connected layer, a ReLu layer, another fully       connected layer, and finally a softmax layer in the order specified.&lt;/li&gt;
      &lt;li&gt;The softmax layer then outputs the probabilities of the pair of candidate questions being similar. Based on the probabilities we classify the questions as         similar or not similar.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/39705529/141838152-b27025fc-becc-47b6-a20f-f6c7bb41e548.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Fig 5: BERT takes in sentences as inputs and gives sentence-level embeddings as the output&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Inference:
    &lt;ul&gt;
      &lt;li&gt;For a query question 𝑄&lt;sub&gt;𝑞&lt;/sub&gt; we identify the cluster 𝑐&lt;sub&gt;𝑖&lt;/sub&gt; it belongs to.&lt;/li&gt;
      &lt;li&gt;For all candidate questions 𝑄&lt;sub&gt;𝑑&lt;/sub&gt; belonging to cluster 𝑐&lt;sub&gt;𝑖&lt;/sub&gt;, we find similarity 𝑠𝑖𝑚(𝑄&lt;sub&gt;𝑑&lt;/sub&gt;, 𝑄&lt;sub&gt;𝑞&lt;/sub&gt;). With clustering we avoid finding similarities with all questions in the question bank, making inference efficient.&lt;/li&gt;
      &lt;li&gt;If for any 𝑄&lt;sub&gt;𝑑&lt;/sub&gt; ε 𝑐&lt;sub&gt;𝑖&lt;/sub&gt;, if 𝑠𝑖𝑚(𝑄&lt;sub&gt;𝑑&lt;/sub&gt;, 𝑄&lt;sub&gt;𝑞&lt;/sub&gt;) &amp;gt; 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑, we flag that question as similar.&lt;/li&gt;
      &lt;li&gt;We also output 𝑡𝑜𝑝 − 𝑘 similar questions based on 𝑠𝑖𝑚(𝑄&lt;sub&gt;𝑑&lt;/sub&gt;, 𝑄&lt;sub&gt;𝑞&lt;/sub&gt;).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Models in consideration:
    &lt;ul&gt;
      &lt;li&gt;Models for learning representations
        &lt;ul&gt;
          &lt;li&gt;BERT&lt;/li&gt;
          &lt;li&gt;GPT&lt;/li&gt;
          &lt;li&gt;XLNet&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Models for clustering
        &lt;ul&gt;
          &lt;li&gt;K-means&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Models for finding similarity
        &lt;ul&gt;
          &lt;li&gt;Feedforward neural network (FFN)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog//images/2021-10-03-quora-question-pairs/similar-questions-flow-chart.png&quot; alt=&quot;&quot; title=&quot;Model in action&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;final-results-and-analysis&quot;&gt;Final results and analysis&lt;/h1&gt;

&lt;h3 id=&quot;supervised-learning-pipeline&quot;&gt;Supervised Learning Pipeline&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Experiment 1 : Data Preprocessing and Augmentation Choices Ablation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Through the first set of experiments, we try to observe the effects of data preprocessing and data augmentation on the performance of the model. We also apply three different types of models on a sample dataset of 1000 data points from the entire original dataset. The results of these are seen in the tables below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Data Preprocessing&lt;/th&gt;
      &lt;th&gt;Data Augmentation&lt;/th&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Train accuracy&lt;/th&gt;
      &lt;th&gt;Train F1 score&lt;/th&gt;
      &lt;th&gt;Test accuracy&lt;/th&gt;
      &lt;th&gt;Test F1 score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Baseline&lt;/td&gt;
      &lt;td&gt;0.57&lt;/td&gt;
      &lt;td&gt;0.61&lt;/td&gt;
      &lt;td&gt;0.42&lt;/td&gt;
      &lt;td&gt;0.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Baseline&lt;/td&gt;
      &lt;td&gt;0.61&lt;/td&gt;
      &lt;td&gt;0.61&lt;/td&gt;
      &lt;td&gt;0.57&lt;/td&gt;
      &lt;td&gt;0.62&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Baseline&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;0.62&lt;/td&gt;
      &lt;td&gt;0.61&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Data Preprocessing&lt;/th&gt;
      &lt;th&gt;Data Augmentation&lt;/th&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Train accuracy&lt;/th&gt;
      &lt;th&gt;Train F1 score&lt;/th&gt;
      &lt;th&gt;Test accuracy&lt;/th&gt;
      &lt;th&gt;Test F1 score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Static BERT&lt;/td&gt;
      &lt;td&gt;0.66&lt;/td&gt;
      &lt;td&gt;0.61&lt;/td&gt;
      &lt;td&gt;0.57&lt;/td&gt;
      &lt;td&gt;0.53&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Static BERT&lt;/td&gt;
      &lt;td&gt;0.67&lt;/td&gt;
      &lt;td&gt;0.62&lt;/td&gt;
      &lt;td&gt;0.59&lt;/td&gt;
      &lt;td&gt;0.57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Static BERT&lt;/td&gt;
      &lt;td&gt;0.66&lt;/td&gt;
      &lt;td&gt;0.62&lt;/td&gt;
      &lt;td&gt;0.59&lt;/td&gt;
      &lt;td&gt;0.55&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Data Preprocessing&lt;/th&gt;
      &lt;th&gt;Data Augmentation&lt;/th&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Train accuracy&lt;/th&gt;
      &lt;th&gt;Train F1 score&lt;/th&gt;
      &lt;th&gt;Test accuracy&lt;/th&gt;
      &lt;th&gt;Test F1 score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Fine-tuned BERT&lt;/td&gt;
      &lt;td&gt;0.98&lt;/td&gt;
      &lt;td&gt;0.97&lt;/td&gt;
      &lt;td&gt;0.57&lt;/td&gt;
      &lt;td&gt;0.41&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Fine-tuned BERT&lt;/td&gt;
      &lt;td&gt;0.97&lt;/td&gt;
      &lt;td&gt;0.97&lt;/td&gt;
      &lt;td&gt;0.58&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Fine-tuned BERT&lt;/td&gt;
      &lt;td&gt;0.97&lt;/td&gt;
      &lt;td&gt;0.96&lt;/td&gt;
      &lt;td&gt;0.64&lt;/td&gt;
      &lt;td&gt;0.52&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here, the baseline model is nothing but the pre-trained BERT-based model, and the embeddings of the sentences obtained from this model are compared using a cosine similarity. This provided us a good baseline result and starting point, which we tried to improve upon.&lt;/p&gt;

&lt;p&gt;For the other two models, the dataset was then split into training and testing data with a ratio of 80:20 respectively. The models were trained over this data for 5 epochs with a learning rate of 0.0001 and batch size of 8. The architecture used for these models in this experiment : &lt;strong&gt;&lt;em&gt;BERT -&amp;gt; FFN -&amp;gt; Sigmoid layer -&amp;gt; FFN -&amp;gt; Softmax layer&lt;/em&gt;&lt;/strong&gt;. In the Static BERT model the pre-trained BERT model is kept static and the remaining layers are trained, whereas in the Fine-tuned BERT, all the layers are trained and the pre-trained BERT-based model is fine-tuned on our dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt; : As we can see from the table, applying data preprocessing techniques (mentioned in section 2) and data augmentation techniques increases the accuracy on the test data. This empirically backs our decision to do data preprocessing and augmentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiment 2 : Fine Tuning Ablation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Through the second set of experiments, we tried to gain insight on which model would work best for our problem. As a part of this, we ran our baseline, Static BERT, and Fine-tuned BERT on a sample dataset of 10000 datapoints randomly chosen from the original dataset. For the Static BERT and Fine-tuned BERT models we again split the data into train and test with a 80:20 ratio, and ran them for 10 epochs with a learning rate of 0.0001 and batch size 32. The architecture used for these 2 models in this experiment : &lt;strong&gt;&lt;em&gt;BERT -&amp;gt; FFN -&amp;gt; PRelu -&amp;gt; FFN -&amp;gt; PRelu -&amp;gt; FFN -&amp;gt; Softmax Layer&lt;/em&gt;&lt;/strong&gt;. The results are seen in the table below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Data Preprocessing&lt;/th&gt;
      &lt;th&gt;Data Augmentation&lt;/th&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Train accuracy&lt;/th&gt;
      &lt;th&gt;Train F1 score&lt;/th&gt;
      &lt;th&gt;Test accuracy&lt;/th&gt;
      &lt;th&gt;Test F1 score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Baseline&lt;/td&gt;
      &lt;td&gt;0.59&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;0.61&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Static BERT&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;0.63&lt;/td&gt;
      &lt;td&gt;0.68&lt;/td&gt;
      &lt;td&gt;0.61&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Fine-tuned BERT&lt;/td&gt;
      &lt;td&gt;0.99&lt;/td&gt;
      &lt;td&gt;0.98&lt;/td&gt;
      &lt;td&gt;0.71&lt;/td&gt;
      &lt;td&gt;0.62&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt; : As it can be seen in the table, the Fine-tuned BERT model produces the best accuracy results on the data as compared to the baseline and Static-BERT. This again empirically backs our decision to choose a pre-trained BERT model and fine-tune it on our dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiment 3 : Model Selection Ablation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We chose to fine-tune our models based on the results from experiment 2. Next, we did some hyperparamter tuning to pick the best combination of hyperparameters for performance. We found that a batch size of 128 and a training over 5 epochs gave the best test results for our Fine-tuned BERT model. We then trained all 400,000 question pairs with this hyperparameter setting. We repeated the same process with GPT and XLNet architectures. The results for this experiment are shown below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Train accuracy&lt;/th&gt;
      &lt;th&gt;Test accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Fine-tuned BERT&lt;/td&gt;
      &lt;td&gt;0.94&lt;/td&gt;
      &lt;td&gt;0.86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Fine-tuned GPT-2&lt;/td&gt;
      &lt;td&gt;0.65&lt;/td&gt;
      &lt;td&gt;0.63&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Fine-tuned XLNet&lt;/td&gt;
      &lt;td&gt;0.64&lt;/td&gt;
      &lt;td&gt;0.64&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt; : As it can be seen in the table, the Fine-tuned BERT model produces the best accuracy results on the test data as compared to the GPT-2 and XLNet models. Thus, we present the Fine-tuned BERT model as our final supervised learning model with a test accuracy of 86%.&lt;/p&gt;

&lt;h3 id=&quot;unsupervised-learning-pipeline-for-efficient-inference&quot;&gt;Unsupervised Learning Pipeline for Efficient Inference&lt;/h3&gt;

&lt;p&gt;We build an unsupervised learning pipeline in order to cluster similar questions. The pipeline consists of 3 steps :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The first part of the pipeline consists of creating BERT embeddings of all the questions after preprocessing them.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We then perform Principal Component Analysis (PCA) in order to reduce the dimensionality of our dataset. From the below graph, we can infer that with just 15 components, 99 percent of the variance is retained.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46374506/144763989-c6abfce0-3003-4b3d-9921-d9aad90937f5.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We cluster the preprocessed dataset using Kmeans clustering algorithm. The elbow method plotted below, is used to find out the ideal number of clusters after analysing the tradeoff between performance and computation cost. From the graph, we can infer that the elbow is at 100 clusters&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46374506/144764219-06373665-f8cb-429a-a9e6-fced14b99b33.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This unsupervised learning pipeline is used to create a smaller question space from which we can choose the candidate reference questions. This in turn reduces the time overhead by 100 times during inference. Here is an example illustrating potential canonical questions:&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46374506/144765011-a35e1759-c413-47be-aa07-d9246e30c682.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this project, we propose a method to classify questions as similar or not similar using a combination of a supervised method and unsupervised method.&lt;/p&gt;

&lt;p&gt;For the supervised method, we implemented multiple transformer based models (such as BERT, GPT-2, and XLNet) as backbones over a feed forward neural network to classify whether two questions are similar or not. After extensive results and ablations we were able to finalize our model as the pre-trained BERT model which is fine-tuned on our data. This supervised model classifies the given query question as similar or not after comparison with the top K reference questions obtained from the unsupervised pipeline based on their sentence embeddings. We achieve a test accuracy of 86% for this task.&lt;/p&gt;

&lt;p&gt;For the unsupervised method, we use K-means clustering to group the questions in the dataset based on their sentence level embeddings, in order to narrow down the question space from which the reference questions are picked for comparison. The unsupervised pipeline gives us the candidate similar questions for each of the clusters. Through this, we were able to form 100 clusters which reduces our overhead significantly.&lt;/p&gt;

&lt;h1 id=&quot;future-experiments&quot;&gt;Future experiments:&lt;/h1&gt;

&lt;p&gt;Future work may include downstream tasks such as automatic question tagging, and personalized recommendation of questions based on the field of interest. This can be very advantageous for forums such as Ed, Quora, Piazza, etc. and can help towards making the work of teaching staff easier.&lt;/p&gt;

&lt;h1 id=&quot;timeline-and-responsibilities&quot;&gt;Timeline and Responsibilities&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2021-10-03-quora-question-pairs/timeline.png&quot; alt=&quot;&quot; title=&quot;Timeline&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs&lt;/li&gt;
  &lt;li&gt;D. A. Prabowo and G. Budi Herwanto, “Duplicate Question Detection in Question Answer Website using Convolutional Neural Network,” 2019 5th International Conference on Science and Technology (ICST), 2019, pp. 1-6, doi: 10.1109/ICST47872.2019.9166343.&lt;/li&gt;
  &lt;li&gt;Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina
Toutanova, “BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding”, arXiv:1810.&lt;/li&gt;
  &lt;li&gt;Tianqi Chen andCarlos Guestrin.2016.XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ‘16). Association for Computing Machinery, New York,NY,USA,785–794.DOI:https://doi.org/10.1145/2939672.785.&lt;/li&gt;
  &lt;li&gt;Reynolds D.(2009)GaussianMixture Models. In:Li S.Z.,Jain A. (eds) Encyclopedia of Biometrics. Springer, Boston, MA. https://doi.org/10.1007/978-0-387-73003-5_&lt;/li&gt;
&lt;/ol&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/abhijithragav/'&gt;Abhijith Ragav&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/ajish-sekar/'&gt;Ajish Sekar&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/naveen-1999/'&gt;Naveen Narayanan&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/pranav-guruprasad-82697514a/'&gt;Pranav Guruprasad&lt;/a&gt;, &lt;a href='https://www.linkedin.com/in/yash-jakhotiya/'&gt;Yash Jakhotiya&lt;/a&gt;</name></author><summary type="html">Course project for CS 7641</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://yashjakhotiya.github.io/blog/images/2021-10-03-quora-question-pairs/similar-questions-flow-chart.png" /><media:content medium="image" url="https://yashjakhotiya.github.io/blog/images/2021-10-03-quora-question-pairs/similar-questions-flow-chart.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Containers, Container Runtimes, and What Kubernetes ‘Docker’ Deprecation Really Means</title><link href="https://yashjakhotiya.github.io/blog/containers/kubernetes/2020/12/20/container-runtimes.html" rel="alternate" type="text/html" title="Containers, Container Runtimes, and What Kubernetes 'Docker' Deprecation Really Means" /><published>2020-12-20T00:00:00-06:00</published><updated>2020-12-20T00:00:00-06:00</updated><id>https://yashjakhotiya.github.io/blog/containers/kubernetes/2020/12/20/container-runtimes</id><content type="html" xml:base="https://yashjakhotiya.github.io/blog/containers/kubernetes/2020/12/20/container-runtimes.html">&lt;h1 id=&quot;containers-arent-they--like-some-sort-of-virtual-machines&quot;&gt;Containers? Aren’t they … like some sort of virtual machines?&lt;/h1&gt;

&lt;p&gt;Docker popularized the notion of using containers - &lt;em&gt;isolated environments leveraging OS-level virtualization&lt;/em&gt; where each running process sees the environment as one whole computer. This seems awfully similar to virtual machines, except that it isn’t. Containers differ from virtual machines in that each container does not host the entire operating system the way virtual machines do.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/containers_vs_vms.png&quot; alt=&quot;&quot; title=&quot;Virtual Machines Vs Containers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Container images&lt;/strong&gt;, which become running containers when instantiated, store the application code and any required dependencies mentioned in the image &lt;a href=&quot;https://docs.docker.com/engine/reference/builder/&quot;&gt;Dockerfile&lt;/a&gt;. When you don’t need to worry about dependencies, shipping applications from a developer’s laptop to production servers or public cloud environments becomes easier. You &lt;em&gt;could&lt;/em&gt; package your application as a custom built VM image relying on a fully functional traditional OS packaged with it, but container images are much more lightweight and can be easily maintained.&lt;/p&gt;

&lt;h1 id=&quot;but-every-dockerfile-i-see-ultimately-stems-from-an-os-image-doesnt-that-mean-container-images-have-their-own-os-installed&quot;&gt;But, every dockerfile I see ultimately stems from an OS image. Doesn’t that mean container images have their own OS installed?&lt;/h1&gt;

&lt;p&gt;This is an excellent question. Thanks for asking! To answer this, let us get some background context.&lt;/p&gt;

&lt;p&gt;In most modern OS, an application process runs in what is known as a &lt;strong&gt;user mode&lt;/strong&gt;. The idea here is to restrict the memory area accessible to an application process and prevent it from accessing and potentially corrupting memory areas associated with kernel and other application processes. This is implemented using &lt;a href=&quot;https://en.wikipedia.org/wiki/Virtual_memory&quot;&gt;Virtual memory&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Protection_ring&quot;&gt;Protection Rings&lt;/a&gt;, and assisted by hardware in the form of &lt;a href=&quot;en.wikipedia.org/wiki/Protected_mode&quot;&gt;Protected mode&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Memory_management_unit&quot;&gt;Memory Management Units&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Providing fault tolerance and computer security with this form of &lt;strong&gt;memory protection&lt;/strong&gt; effectively results in a typical OS being divided into two bifurcations - a user space and a kernel space. An application running in user space can access resources which it does not have direct access to (like I/O devices or files lying on a disk) with special requests to the kernel called &lt;a href=&quot;https://man7.org/linux/man-pages/man2/syscalls.2.html&quot;&gt;system calls&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/user_space_kernel_space.png&quot; alt=&quot;&quot; title=&quot;A process in user space makes a system call&quot; /&gt;&lt;/p&gt;

&lt;p&gt;System calls serve as APIs for all &lt;strong&gt;userland&lt;/strong&gt; software to interact with the kernel. In the Linux world, all distros (a bit simplification here) run the same kernel. This makes it possible for the &lt;em&gt;userland software coming from Ubuntu to talk to a CentOS kernel&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;What you see inside a Dockerfile, which gets installed in the built image, is &lt;strong&gt;not a full-fledged OS&lt;/strong&gt;. It is the trimmed-down version of the userland software of the OS, &lt;em&gt;bare enough to talk to the host’s kernel&lt;/em&gt;. It is not uncommon to see containers with Ubuntu, CentOS, and Debian base run parallely on a RHEL7 host.&lt;/p&gt;

&lt;h1 id=&quot;ok-i-am-curious-how-is-this-implemented&quot;&gt;Ok. I am curious. How is this implemented?&lt;/h1&gt;

&lt;p&gt;To be honest, container implementation recipe is really not that difficult if you understand its three main ingredients - cgroups, namespaces and chroot. Let us focus on each of them below.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;cgroups&lt;/strong&gt;, or Control Groups is a Linux kernel feature. With cgroups you can &lt;em&gt;allocate, monitor, and limit resources&lt;/em&gt; - like CPU time, memory, or network bandwidth - to a process or a collection of processes. Linux command &lt;a href=&quot;https://linux.die.net/man/1/cgcreate&quot;&gt;cgcreate&lt;/a&gt; helps you create a control group, &lt;a href=&quot;https://linux.die.net/man/1/cgset&quot;&gt;cgset&lt;/a&gt; sets resource limits for the control group, and with &lt;a href=&quot;https://linux.die.net/man/1/cgexec&quot;&gt;cgexec&lt;/a&gt; you can run a command in the control group.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;namespace&lt;/strong&gt; is another Linux kernel feature, with which you can &lt;em&gt;isolate a global resource&lt;/em&gt;. This creates an illusion of a separate instance of the resource to the processes running in the namespace, and any changes made are &lt;strong&gt;not visible outside&lt;/strong&gt;! The resources you can abstract this way include process IDs, hostnames, user IDs, etc. &lt;a href=&quot;https://man7.org/linux/man-pages/man1/unshare.1.html&quot;&gt;unshare&lt;/a&gt; [options] [&lt;em&gt;program&lt;/em&gt; [arguments]] is a Linux utility you can use to create namespaces (supplied in &lt;code class=&quot;highlighter-rouge&quot;&gt;options&lt;/code&gt;) and run &lt;code class=&quot;highlighter-rouge&quot;&gt;program&lt;/code&gt; in it. For example you can create a UTS (Unix Time Sharing) namespace, which controls host and domain names, using the -u option as illustrated below.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; hostname                              # show current hostname 
personal-ubuntu
&amp;gt; unshare -u /bin/sh                    # run a shell instance with UTS namespace unshared from parent
&amp;gt; hostname a-different-hostname         # change hostname to a-different-hostname
&amp;gt; hostname                              # verify that the hostname has been changed
a-different-hostname
&amp;gt; exit                                  # exit from the shell process, effectively destroying the namespace
&amp;gt; hostname                              # voila!
personal-ubuntu                         # changing the hostname inside the namespace has no effect outside!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://linux.die.net/man/1/chroot&quot;&gt;chroot&lt;/a&gt; is a Linux utility that can &lt;em&gt;change the apparent root directory for a process and its children&lt;/em&gt;. Running &lt;code class=&quot;highlighter-rouge&quot;&gt;chroot NEWROOT command&lt;/code&gt; will run &lt;code class=&quot;highlighter-rouge&quot;&gt;command&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;NEWROOT&lt;/code&gt; as its apparent root directory. This modified environment is also called a &lt;strong&gt;chroot jail&lt;/strong&gt;, because &lt;code class=&quot;highlighter-rouge&quot;&gt;command&lt;/code&gt; can not name and hence can not normally access files outside &lt;code class=&quot;highlighter-rouge&quot;&gt;NEWROOT&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that you have understood these three main concepts, let us create a container image from the following dockerfile, which we want to run.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM ubuntu:18.04
COPY script.py /app
CMD python /app/script.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This container image contains the ubuntu:18.04 userland file structure heirarchy, /app/script.py and some environment configuration. Ignoring the config part for now, &lt;strong&gt;your minimal implementation can run this image in just 4 steps&lt;/strong&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Export and extract contents of the image in &lt;code class=&quot;highlighter-rouge&quot;&gt;new_root_dir&lt;/code&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; mkdir new_root_dir
&amp;gt; docker export docker_image | tar -xf - -C new_root_dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Create a control group and set memory and CPU use limits
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; control_group=$(uuidgen)
&amp;gt; cgcreate -g cpu,memory:$control_group
&amp;gt; cgset -r memory.limit_in_bytes=50000000 $control_group
&amp;gt; cgset -r cpu.shares=256 $control_group
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Executing inside the control group, call unshare to separate namespaces and execute &lt;code class=&quot;highlighter-rouge&quot;&gt;script.py&lt;/code&gt; inside the &lt;code class=&quot;highlighter-rouge&quot;&gt;new_root_dir&lt;/code&gt; jail
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; cgexec -g cpu,memory:$control_group unshare -uinpUrf --mount-proc sh -c &quot;chroot new_root_dir /app/script.py&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Cleanup. Delete the cgroup and &lt;code class=&quot;highlighter-rouge&quot;&gt;new_root_dir&lt;/code&gt;. Unless bound to a file, namespaces cease to exist once all running processes in the namespace have exited.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; cgdelete -r -g cpu,memory:$control_group
&amp;gt; rm -r new_root_dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Lo and behold!&lt;/strong&gt; You have just created a minimal container runtime!&lt;/p&gt;

&lt;h1 id=&quot;whoa-wait-container-r-what&quot;&gt;Whoa! Wait, Container R… what?&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Container Runtime&lt;/strong&gt; - the code and tooling responsible for running containers. What you created above is the heart of what every container runtime does. Although, it catches the essence of container runtimes, it’s still minimal. Docker images also have something known as a &lt;code class=&quot;highlighter-rouge&quot;&gt;config.json&lt;/code&gt;. This file has, among other things, environment variables to be set for the running process inside the container, and the uid and gid of the user the process must run as.&lt;/p&gt;

&lt;p&gt;The code to run containers used to be deep inside a monolith called &lt;code class=&quot;highlighter-rouge&quot;&gt;Docker&lt;/code&gt;. But, it need not be. As long as vendors agree upon a common specification for images and a common specification for runtimes, &lt;em&gt;anybody could create runtimes&lt;/em&gt; customized to their needs. &lt;strong&gt;That’s exactly what they did&lt;/strong&gt;. Docker, CoreOS, Google and other industry leaders in the container space came together and launched &lt;a href=&quot;https://opencontainers.org&quot;&gt;Open Container Initiative&lt;/a&gt; in June 2015. OCI is responsible for defining &lt;a href=&quot;https://github.com/opencontainers/image-spec&quot;&gt;image-spec&lt;/a&gt; and &lt;a href=&quot;https://github.com/opencontainers/runtime-spec&quot;&gt;runtime-spec&lt;/a&gt;, which every OCI-compliant image builder and container runtime has to abide by.&lt;/p&gt;

&lt;p&gt;OCI even develops and maintains a reference implementation of the runtime-spec called &lt;a href=&quot;https://github.com/opencontainers/runc&quot;&gt;runc&lt;/a&gt;. runc broke off from Docker, as part of the Open Container Initiative. Although, runc is self-sufficient to run containers, it is a low-level runtime. The only developers that work with runc are developers of high-level runtimes.&lt;/p&gt;

&lt;h1 id=&quot;come-on-these-container-runtimes-have-levels-now&quot;&gt;Come on! These container runtimes have ‘levels’ now?&lt;/h1&gt;

&lt;p&gt;Yes, they very much do! If you ever used Docker, you might know that running containers from images isn’t all that you do. You might want to &lt;strong&gt;pull images&lt;/strong&gt; from registries before you actually run them. A higher level runtime does that for you.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/container_runtimes.png&quot; alt=&quot;&quot; title=&quot;A higher level runtime interacting with a lower level runtime&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Higher level runtimes are also responsible for &lt;strong&gt;unpacking&lt;/strong&gt; the container image into an &lt;a href=&quot;https://github.com/opencontainers/runtime-spec/blob/master/bundle.md&quot;&gt;OCI runtime bundle&lt;/a&gt; before spawning a runc process to run it. In addition to managing the lifecycle of a container, higher level runtimes are also sometimes responsible for low level storage and network namespace management. &lt;strong&gt;This is usually in place to facilitate interaction between individual container processes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Humans aren’t the only entities that interact with higher level runtimes. &lt;a href=&quot;https://www.redhat.com/en/topics/containers/what-is-container-orchestration&quot;&gt;Container orchestration&lt;/a&gt; services (&lt;em&gt;just a fancy term for management and configuration of containers across large dynamic systems&lt;/em&gt;), like &lt;a href=&quot;https://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt;, need to interact with high-level runtimes. For most industry use-cases, it’s less humans and more such services that talk to these runtimes.&lt;/p&gt;

&lt;h1 id=&quot;did-you-mention-kubernetes-you-had-my-curiosity-now-you-have-my-attention&quot;&gt;Did you mention Kubernetes? You had my curiosity. Now you have my attention.&lt;/h1&gt;

&lt;p&gt;What interacts with high-level container runtimes are not client-facing modules of a running Kubernetes instance, &lt;strong&gt;but a node-agent called&lt;/strong&gt; &lt;a href=&quot;https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/&quot;&gt;kubelet&lt;/a&gt; which runs on all nodes in a Kubernetes cluster. &lt;strong&gt;Kubelet&lt;/strong&gt; is responsible to ensure all containers mentioned in a pod’s specification are running and healthy. It registers nodes, sends pod status and events, and reports resource utilization higher up the command chain.&lt;/p&gt;

&lt;p&gt;With the introduction of OCI, many container runtimes came up that supported running OCI-compliant container images, and &lt;strong&gt;so arised the need for Kubernetes to support multiple runtimes&lt;/strong&gt;. To avoid deep integration of such runtimes into kubelet source code, and the subsequent maintenance that would follow, Kubernetes introduced the &lt;a href=&quot;https://github.com/kubernetes/cri-api&quot;&gt;Container Runtime Interface&lt;/a&gt; - &lt;em&gt;an interface definition which enables kubelet to use a wide variety of runtimes&lt;/em&gt;. It is the responsibility of a container runtime to implement this interface as an internal package or as a &lt;strong&gt;shim&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/containerd&quot;&gt;containerd&lt;/a&gt;, a prominent high-level container runtime, which broke off from Docker similar to runc, recently merged its separate &lt;a href=&quot;https://github.com/containerd/cri&quot;&gt;cri-plugin&lt;/a&gt; codebase to its main &lt;a href=&quot;https://github.com/containerd/containerd&quot;&gt;containerd/containerd&lt;/a&gt; repository, &lt;strong&gt;marking CRI-implementation to be an important part of the container runtime&lt;/strong&gt;. &lt;a href=&quot;cri-o.io&quot;&gt;cri-o&lt;/a&gt; is another implementation of CRI, &lt;em&gt;focused and optimized only for Kubernetes&lt;/em&gt;, and, unlike containerd, can not service docker daemons for container orchestration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/kubelet_to_kernel.png&quot; alt=&quot;&quot; title=&quot;Kubelet to Kernel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that we have established CRI, let us talk about what the recent &lt;strong&gt;Kubernetes Docker Deprecation&lt;/strong&gt; really means.&lt;/p&gt;

&lt;h1 id=&quot;finally&quot;&gt;Finally!&lt;/h1&gt;

&lt;p&gt;Kuberenetes recently announced that it would be &lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation&quot;&gt;deprecating Docker&lt;/a&gt;. It really isn’t as dramatic as it sounds. What Kuberenetes will not support is &lt;strong&gt;Docker as a runtime&lt;/strong&gt;, and nothing else changes. Images built with dockerfiles are OCI-compliant and hence can be very well used with Kubernetes. Both containerd and cri-o know how to pull them, and runc knows how to run them.&lt;/p&gt;

&lt;p&gt;Docker, being built for human interaction, isn’t really friendly for Kubernetes as just a runtime. To interact with it, Kubernetes has to develop a module called &lt;strong&gt;dockershim&lt;/strong&gt;, which implements CRI support for Docker. This makes Docker call-able by kubelet as a runtime. Kubernetes is no longer willing to maintain this, especially when containerd (which Docker internally uses) has a CRI plugin. If you are developer, you do not really need to worry about what runtimes kubelet can interact with. &lt;strong&gt;Docker built images are perfectly fine for Kubernetes to consume!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/dockershim_containerd.png&quot; alt=&quot;&quot; title=&quot;Dockershim deprecation&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;end-notes&quot;&gt;End Notes&lt;/h1&gt;

&lt;p&gt;I hope you liked reading this blog post as much as I loved writing it. I’ll soon update a &lt;em&gt;large&lt;/em&gt; list of references which can be used for further reading. In the meanwhile, please feel free to follow me on &lt;a href=&quot;https://twitter.com/yash_jakhotiya&quot;&gt;Twitter&lt;/a&gt; and subscribe to the Blog’s &lt;a href=&quot;https://yashjakhotiya.github.io/blog/feed.xml&quot;&gt;RSS Feed&lt;/a&gt; for further updates. For any &lt;strong&gt;feedback or suggestions for blog posts&lt;/strong&gt;, please drop an &lt;a href=&quot;mailto:mailsforyashj@gmail.com&quot;&gt;email&lt;/a&gt; or DM on Twitter. Thanks for reading!&lt;/p&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/yash-jakhotiya/'&gt;Yash Jakhotiya&lt;/a&gt;</name></author><summary type="html">Containers? Aren’t they … like some sort of virtual machines?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/runc_in_docker.png" /><media:content medium="image" url="https://yashjakhotiya.github.io/blog/images/2020-12-20-container-runtimes/runc_in_docker.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">GSoC ‘20: Kubeflow Customer User Journey Notebooks with Tensorflow 2.x Keras</title><link href="https://yashjakhotiya.github.io/blog/open-source/mlops/2020/08/23/gsoc-kubeflow.html" rel="alternate" type="text/html" title="GSoC '20: Kubeflow Customer User Journey Notebooks with Tensorflow 2.x Keras" /><published>2020-08-23T00:00:00-05:00</published><updated>2020-08-23T00:00:00-05:00</updated><id>https://yashjakhotiya.github.io/blog/open-source/mlops/2020/08/23/gsoc-kubeflow</id><content type="html" xml:base="https://yashjakhotiya.github.io/blog/open-source/mlops/2020/08/23/gsoc-kubeflow.html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Open source software development and &lt;a href=&quot;https://summerofcode.withgoogle.com/&quot;&gt;Google Summer of Code&lt;/a&gt;, both started long before the summer of 2020. When the world was starting to grapple with the realities of &lt;a href=&quot;https://www.entrepreneur.com/article/354872&quot;&gt;remote work&lt;/a&gt;, open source community was already thriving on it. Over the course of my college years, I have found out three things that I am passionate about - open source, machine learning and &lt;a href=&quot;https://landing.google.com/sre/&quot;&gt;SRE&lt;/a&gt;. &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow&lt;/a&gt; has managed to incorporate all of these into one and doing a project with this organisation has been a dream come true!&lt;/p&gt;

&lt;h1 id=&quot;goal&quot;&gt;Goal&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt; is already an industry-standard in managing cloud resources. &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow&lt;/a&gt; is on its path to become an industry standard in managing machine learning workflows on cloud. Examples that illustrate Kubeflow functionalities using latest industry technologies make Kubeflow easier to use and more accessible to all potential users. This project has aimed at building samples for Jupyter notebook to Kubeflow deployment using Tensorflow 2.0 Keras for backend training code, illustrating customer user journey (CUJ) in the process. This project has also served as an hands-on to large scale application of machine learning bringing in the elements of DevOps and SRE and this has kept me motivated throughout the project.&lt;/p&gt;

&lt;h1 id=&quot;the-kubeflow-community&quot;&gt;The Kubeflow Community&lt;/h1&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.kubeflow.org/docs/about/community/&quot;&gt;Kubeflow community&lt;/a&gt; is a highly approachable and closely-knit community that has been reaching out to and &lt;a href=&quot;https://www.kubeflow.org/docs/about/gsoc/&quot;&gt;helping&lt;/a&gt; potential GSoC students well before the application period. Respecting this, I made sure to take feedback for my proposal of the &lt;a href=&quot;https://summerofcode.withgoogle.com/projects/#5507335985823744&quot;&gt;project idea&lt;/a&gt; I chose, before the application deadline. Mentors &lt;a href=&quot;https://github.com/terrytangyuan&quot;&gt;Yuan Tang&lt;/a&gt;, &lt;a href=&quot;https://github.com/gaocegege&quot;&gt;Ce Gao&lt;/a&gt; and &lt;a href=&quot;https://github.com/ChanYiLin&quot;&gt;Jack Lin&lt;/a&gt; were candid in providing me feedback and I refined and changed my proposal accordingly. To my sweet surprise, I got selected for the idea!😁 What has really helped me in these three months of coding period is that one month of &lt;a href=&quot;https://developers.google.com/open-source/gsoc/timeline&quot;&gt;community bonding&lt;/a&gt; where I got to know the community and more about the technicalities of Kubeflow.&lt;/p&gt;

&lt;h1 id=&quot;the-project&quot;&gt;The Project&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://yashjakhotiya.github.io/blog/images/2020-08-23-gsoc-20-tf-2-examples/kubeflow_components.png&quot; alt=&quot;&quot; title=&quot;Kubeflow Components&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Examples created as part of this project needed to be easily reproducible to serve their purpose. Initially the underlying model decided to demonstrate Kubeflow functionalities was a BiDirectional RNN to be trained on IMDB large movie review &lt;a href=&quot;http://ai.stanford.edu/%7Eamaas/data/sentiment/&quot;&gt;dataset&lt;/a&gt; for sentiment analysis based on a &lt;a href=&quot;https://www.tensorflow.org/tutorials/text/text_classification_rnn&quot;&gt;tensorflow tutorial&lt;/a&gt;. Over the course of time, we decided to also add another set of examples using  a neural machine translation model in its backend trained on a Spanish to English &lt;a href=&quot;http://www.manythings.org/anki/&quot;&gt;dataset&lt;/a&gt; based on another &lt;a href=&quot;https://www.tensorflow.org/tutorials/text/nmt_with_attention&quot;&gt;tensorflow tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The reasons for choosing these models were:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/kubeflow/examples&quot;&gt;kubeflow/examples&lt;/a&gt; repo needed more NLP-related tasks.&lt;/li&gt;
  &lt;li&gt;These were more of &lt;em&gt;hello world&lt;/em&gt; tasks in the field of NLP. So that users who go through these samples need not worry about training code and focus more on Kubeflow’s functionalities.&lt;/li&gt;
  &lt;li&gt;These are based on tensorflow tutorials. Kubeflow tutorials based on Tensorflow tutorials show better coupling between the two.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;repository-structure&quot;&gt;Repository Structure&lt;/h2&gt;

&lt;p&gt;I created a &lt;a href=&quot;https://github.com/yashjakhotiya/kubeflow-gsoc-2020&quot;&gt;repo&lt;/a&gt; under my own profile to regularly push commits to and my mentors consistently reviewed the work I pushed there. This repo has all of my work with the log history preserved. Each of the two models has the following folder structure explaining core Kubeflow functionalities -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;training-model&amp;gt;.py&lt;/code&gt; - This is the core training code upon which all subsequent examples showing Kubeflow functionalities are based. Please go through this first to know more about the machine learning task subsequent notebooks will manage. For example check the &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/text_classification_rnn.py&quot;&gt;source code&lt;/a&gt; of the model used for the text classification task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;distributed_&amp;lt;training-model&amp;gt;.py&lt;/code&gt; - To truly take advantage of multiple compute nodes, the training code has to be modified to support distributed training. The code in the above mentioned file is modified with Tensorflow’s &lt;a href=&quot;https://www.tensorflow.org/guide/distributed_training&quot;&gt;distributed training&lt;/a&gt; strategy and hosted &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/distributed_text_classification_rnn.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; - This is the dockerfile which is used to build Docker image of the training code. Some Kubeflow functionalities require that a docker image of the training code is built and hosted on a docker container registry. This Docker 101 &lt;a href=&quot;https://www.docker.com/101-tutorial&quot;&gt;tutorial&lt;/a&gt; is a good starting point to get hands-on training on Docker. For complete starters in the field of containerization, this &lt;a href=&quot;https://opensource.com/resources/what-docker&quot;&gt;introduction&lt;/a&gt; can serve as a good starting point. The dockerfile used with the source code mentioned above can be found &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/Dockerfile&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fairing-with-python-sdk.ipynb&lt;/code&gt; - Fairing is a Kubeflow functionality that lets you run model training tasks remotely. This is the Jupyter notebook which deploys a model training task on cloud using Kubeflow Fairing. Fairing does not require you to build a Docker image of the training code first. Hence, its training code resides in the same notebook. To know more about Kubeflow Fairing, please visit Fairing’s &lt;a href=&quot;https://www.kubeflow.org/docs/components/fairing/fairing-overview/&quot;&gt;official documentation&lt;/a&gt;. To get a better idea about Fairing, you can take a look at the text classification Fairing notebook &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/fairing-with-python-sdk.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;katib-with-python-sdk.ipynb&lt;/code&gt; - &lt;a href=&quot;https://www.kubeflow.org/docs/components/hyperparameter-tuning/hyperparameter/&quot;&gt;Katib&lt;/a&gt; is a Kubeflow functionality that lets you perform hyperparameter tuning experiments and reports best set of hyperparameters based on a provided metric. This is the Jupyter notebook which launches Katib hyperparameter tuning experiments using its &lt;a href=&quot;https://github.com/kubeflow/katib/tree/master/sdk/python/v1alpha3&quot;&gt;Python SDK&lt;/a&gt;. Katib requires you to build and host a Docker image of your training code in a container registry. For this sample, we have used &lt;a href=&quot;https://cloud.google.com/cloud-build/docs&quot;&gt;gcloud builds&lt;/a&gt; to build the required Docker image of the training code along with the training data and host it on &lt;a href=&quot;https://cloud.google.com/container-registry&quot;&gt;Google Container Registry&lt;/a&gt;. This is the &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/katib-with-python-sdk.ipynb&quot;&gt;notebook&lt;/a&gt; we used to demonstrate Katib for the text classification task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tfjob-with-python-sdk.ipynb&lt;/code&gt; - &lt;a href=&quot;https://www.kubeflow.org/docs/components/training/tftraining/&quot;&gt;TFJobs&lt;/a&gt; are used to run distributed training jobs over Kubernetes. With multiple workers, TFJob truly leverage the ability of your code to support distributed training. This Jupyter notebook demonstrates how to use TFJob. The Docker image built from the distributed version of our core training code is used in this notebook. TFJob notebook for the text classification task can be found &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/tfjob-with-python-sdk.ipynb&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tekton-pipeline-with-python-sdk.ipynb&lt;/code&gt; - &lt;a href=&quot;https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/&quot;&gt;Kubeflow Pipeline&lt;/a&gt; is a platform that lets you build, manage and deploy end-to-end machine learning workflows. This is a Jupyter notebook which bundles Katib hyperparameter tuning and TFJob distributed training into one Kubeflow pipeline. The pipeline used here uses &lt;a href=&quot;https://cloud.google.com/tekton&quot;&gt;Tekton&lt;/a&gt; in its backend. Tekton is a Kubernetes resource to create efficient &lt;a href=&quot;https://opensource.com/article/18/8/what-cicd&quot;&gt;continuous integration and delivery&lt;/a&gt; (CI/CD) systems. The pipeline notebook for the text classification task can be found at &lt;a href=&quot;https://github.com/kubeflow/examples/blob/master/tensorflow_cuj/text_classification/tekton-pipeline-with-python-sdk.ipynb&quot;&gt;this&lt;/a&gt; place.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;merged-prs&quot;&gt;Merged PRs&lt;/h2&gt;

&lt;p&gt;In the community bonding period, I opened numerous small issues and PRs solving these issues, as I encountered them when reading documentation or implementing an example. This was done in an effort to know more about the Kubeflow community.&lt;/p&gt;

&lt;p&gt;For the main project, I copied these built notebooks and the final work product into a directory created in my fork of the &lt;a href=&quot;https://github.com/kubeflow/examples&quot;&gt;kubeflow/examples&lt;/a&gt; repo and created a &lt;a href=&quot;https://github.com/kubeflow/examples/pull/816&quot;&gt;PR&lt;/a&gt; to add these notebooks in Kubeflow’s official repo. The PR got merged and the code currently resides in &lt;a href=&quot;https://github.com/kubeflow/examples/tree/master/tensorflow_cuj&quot;&gt;kubeflow/examples/tensorflow_cuj&lt;/a&gt; directory marking the completion of the project.&lt;/p&gt;

&lt;h1 id=&quot;special-thanks&quot;&gt;Special Thanks&lt;/h1&gt;

&lt;p&gt;Special thanks are due to -&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;My mentors for their valuable guidance throughout the project.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/jeremy-lewi-600aaa8/&quot;&gt;Jeremy&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/sarahmaddox/&quot;&gt;Sarah&lt;/a&gt; for smooth conduction of the Kubeflow GSoC program.&lt;/li&gt;
  &lt;li&gt;The GSoC Discord &lt;a href=&quot;https://discord.com/channels/708636399666069514/708636400097951744&quot;&gt;Server&lt;/a&gt; and the GSoC Telegram &lt;a href=&quot;https://web.telegram.org/#/im?p=s1263176603_5411849872541551939&quot;&gt;Channel&lt;/a&gt; for the help, casual talks and a strong global student community.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>&lt;a href='https://www.linkedin.com/in/yash-jakhotiya/'&gt;Yash Jakhotiya&lt;/a&gt;</name></author><summary type="html">Introduction</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://yashjakhotiya.github.io/blog/images/2020-08-23-gsoc-20-tf-2-examples/logos.jpg" /><media:content medium="image" url="https://yashjakhotiya.github.io/blog/images/2020-08-23-gsoc-20-tf-2-examples/logos.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>