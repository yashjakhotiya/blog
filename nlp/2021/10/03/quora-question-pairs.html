<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Can we avoid repeated questions by identifying similar intent? | Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Can we avoid repeated questions by identifying similar intent?" />
<meta name="author" content="<a href='https://www.linkedin.com/in/abhijithragav/'>Abhijith Ragav</a>, <a href='https://www.linkedin.com/in/ajish-sekar/'>Ajish Sekar</a>, <a href='https://www.linkedin.com/in/naveen-1999/'>Naveen Narayanan</a>, <a href='https://www.linkedin.com/in/pranav-guruprasad-82697514a/'>Pranav Guruprasad</a>, <a href='https://www.linkedin.com/in/yash-jakhotiya/'>Yash Jakhotiya</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Inhibiting repeated questions can help avoid duplication of efforts" />
<meta property="og:description" content="Inhibiting repeated questions can help avoid duplication of efforts" />
<link rel="canonical" href="https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html" />
<meta property="og:url" content="https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html" />
<meta property="og:site_name" content="Blog" />
<meta property="og:image" content="https://yashjakhotiya.github.io/blog/images/2021-10-03-quora-question-pairs/similar-questions-flow-chart.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-03T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"<a href='https://www.linkedin.com/in/abhijithragav/'>Abhijith Ragav</a>, <a href='https://www.linkedin.com/in/ajish-sekar/'>Ajish Sekar</a>, <a href='https://www.linkedin.com/in/naveen-1999/'>Naveen Narayanan</a>, <a href='https://www.linkedin.com/in/pranav-guruprasad-82697514a/'>Pranav Guruprasad</a>, <a href='https://www.linkedin.com/in/yash-jakhotiya/'>Yash Jakhotiya</a>"},"description":"Inhibiting repeated questions can help avoid duplication of efforts","@type":"BlogPosting","headline":"Can we avoid repeated questions by identifying similar intent?","dateModified":"2021-10-03T00:00:00-05:00","datePublished":"2021-10-03T00:00:00-05:00","image":"https://yashjakhotiya.github.io/blog/images/2021-10-03-quora-question-pairs/similar-questions-flow-chart.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html"},"url":"https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://yashjakhotiya.github.io/blog/feed.xml" title="Blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-176702229-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16.png">
<link rel="manifest" href="/blog/images/site.webmanifest">
<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Can we avoid repeated questions by identifying similar intent? | Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Can we avoid repeated questions by identifying similar intent?" />
<meta name="author" content="<a href='https://www.linkedin.com/in/abhijithragav/'>Abhijith Ragav</a>, <a href='https://www.linkedin.com/in/ajish-sekar/'>Ajish Sekar</a>, <a href='https://www.linkedin.com/in/naveen-1999/'>Naveen Narayanan</a>, <a href='https://www.linkedin.com/in/pranav-guruprasad-82697514a/'>Pranav Guruprasad</a>, <a href='https://www.linkedin.com/in/yash-jakhotiya/'>Yash Jakhotiya</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Inhibiting repeated questions can help avoid duplication of efforts" />
<meta property="og:description" content="Inhibiting repeated questions can help avoid duplication of efforts" />
<link rel="canonical" href="https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html" />
<meta property="og:url" content="https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html" />
<meta property="og:site_name" content="Blog" />
<meta property="og:image" content="https://yashjakhotiya.github.io/blog/images/2021-10-03-quora-question-pairs/similar-questions-flow-chart.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-03T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"<a href='https://www.linkedin.com/in/abhijithragav/'>Abhijith Ragav</a>, <a href='https://www.linkedin.com/in/ajish-sekar/'>Ajish Sekar</a>, <a href='https://www.linkedin.com/in/naveen-1999/'>Naveen Narayanan</a>, <a href='https://www.linkedin.com/in/pranav-guruprasad-82697514a/'>Pranav Guruprasad</a>, <a href='https://www.linkedin.com/in/yash-jakhotiya/'>Yash Jakhotiya</a>"},"description":"Inhibiting repeated questions can help avoid duplication of efforts","@type":"BlogPosting","headline":"Can we avoid repeated questions by identifying similar intent?","dateModified":"2021-10-03T00:00:00-05:00","datePublished":"2021-10-03T00:00:00-05:00","image":"https://yashjakhotiya.github.io/blog/images/2021-10-03-quora-question-pairs/similar-questions-flow-chart.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html"},"url":"https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://yashjakhotiya.github.io/blog/feed.xml" title="Blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-176702229-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<script src="https://hypothes.is/embed.js" async></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Can we avoid repeated questions by identifying similar intent?</h1><p class="page-description">Inhibiting repeated questions can help avoid duplication of efforts</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-03T00:00:00-05:00" itemprop="datePublished">
        Oct 3, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name"><a href='https://www.linkedin.com/in/abhijithragav/'>Abhijith Ragav</a>, <a href='https://www.linkedin.com/in/ajish-sekar/'>Ajish Sekar</a>, <a href='https://www.linkedin.com/in/naveen-1999/'>Naveen Narayanan</a>, <a href='https://www.linkedin.com/in/pranav-guruprasad-82697514a/'>Pranav Guruprasad</a>, <a href='https://www.linkedin.com/in/yash-jakhotiya/'>Yash Jakhotiya</a></span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#nlp">nlp</a>
        
      
      </p>
    

    </header>

  <script data-ad-client="ca-pub-7392888799974730" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#introductionbackground">Introduction/Background</a></li>
<li class="toc-entry toc-h1"><a href="#problem-definition">Problem Definition</a></li>
<li class="toc-entry toc-h1"><a href="#methods">Methods</a></li>
<li class="toc-entry toc-h1"><a href="#final-results-and-analysis">Final results and analysis</a>
<ul>
<li class="toc-entry toc-h3"><a href="#supervised-learning-pipeline">Supervised Learning Pipeline</a></li>
<li class="toc-entry toc-h3"><a href="#unsupervised-learning-pipeline-for-efficient-inference">Unsupervised Learning Pipeline for Efficient Inference</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#conclusion">Conclusion</a></li>
<li class="toc-entry toc-h1"><a href="#future-experiments">Future experiments</a></li>
<li class="toc-entry toc-h1"><a href="#timeline-and-responsibilities">Timeline and Responsibilities</a></li>
<li class="toc-entry toc-h1"><a href="#references">References</a></li>
</ul><p><em>Course project for <a href="https://mahdi-roozbahani.github.io/CS46417641-fall2021/">CS 7641</a></em></p>

<h1 id="introductionbackground">
<a class="anchor" href="#introductionbackground" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction/Background</h1>

<p>With public discussion forums becoming highly popular in recent times, especially with classes and work moving to an online setup, websites like Edstem, Piazza, Stack Overflow and Quora have a constant influx of a large volume of questions. Multiple questions with the same intent leads to redundancy, and waste of time, space and resources.</p>

<h1 id="problem-definition">
<a class="anchor" href="#problem-definition" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problem Definition</h1>

<p>The aim is to identify and flag questions with a high similarity index, and retain only canonical questions in order to make the work of administrative staff such as TAs/Professors easier in terms of answering repeated questions or questions of the same intent.</p>

<h1 id="methods">
<a class="anchor" href="#methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Methods</h1>

<ol>
  <li>Dataset: The dataset that we will be using is the Quora Question Pairs dataset<sup>[1]</sup>. It consists of 404,290 pairs of questions. Each datapoint consists of a pair of questions and whether or not they are similar.</li>
</ol>

<p>In order to gain useful insights, we visualize and understand patterns in our data using the following plots.</p>

<p><img src="https://user-images.githubusercontent.com/22400185/141877000-0663d2d9-2a99-4f59-b7ee-1cd367323457.png" alt="WordCloud">
<em>Fig 1: Word Cloud depicting which words appear more frequently in the dataset</em></p>

<p><img src="https://user-images.githubusercontent.com/22400185/141876903-478cd635-ae3a-45c8-a491-7f71b0c14011.jpeg" alt="Histogram of character count">
<em>Fig 2: Histogram representing the number of characters in each question</em></p>

<p><img src="https://user-images.githubusercontent.com/22400185/141876911-18ff1378-cb7a-411c-b82b-f13c571eddd2.jpeg" alt="Histogram of word count">
<em>Fig 3: Histogram representing the number of words in each question</em></p>

<p><img src="https://user-images.githubusercontent.com/22400185/141876917-4d315806-63bb-4a46-a7a4-c55c6acff8ac.jpeg" alt="Word share"></p>

<p><em>Fig 4: Plots representing the distribution of ratio of words shared between similar and dissimlar question</em></p>

<ol>
  <li>Data Augmentation:
    <ul>
      <li>We exploit the transitive property of similarity to generate new datapoints.
        <ul>
          <li>If Question Q<sub>1</sub> is similar to Question Q<sub>2</sub> and Question Q<sub>2</sub> is similar to Question Q<sub>3</sub>, then we can infer that Q<sub>1</sub> is similar to Q<sub>3</sub>
</li>
          <li>If Question Q<sub>1</sub> is similar to Question Q<sub>2</sub> and Question Q<sub>2</sub> is not similar to Question Q<sub>3</sub>, then we can infer that Q<sub>1</sub> is not similar to Q<sub>3</sub>
</li>
        </ul>
      </li>
    </ul>

    <p>Through this method, we generated 25% more training datapoints.</p>
  </li>
  <li>Data preprocessing:
    <ul>
      <li>We used the following preprocessing techniques:
        <ul>
          <li>Sample Sentence: “How can internet speed be increased by hacking through DNS?”
            <ol>
              <li>Removing punctuation</li>
            </ol>
            <ul>
              <li>After Punctuation Removal: “How can internet speed be increased by hacking through DNS”
          2. Converting characters to Lower Case</li>
              <li>After conversion: “how can internet speed be increased by hacking through dns”
          3. Tokenization</li>
              <li>Tokenization is the process of converting the sentence into a list of it’s constituent words/phrases. It helps reduce the input to a finite set, making it easier to process.</li>
              <li>After Tokenization: [‘how’, ‘can’, ‘internet’, ‘speed’, ‘be’, ‘increased’, ‘by’, ‘hacking’, ‘through’, ‘dns’]
          4. Stop Word Removal</li>
              <li>The idea of Stop Word Removal is to remove commonly occuring words in the corpus.</li>
              <li>After Stop Word Removal: [‘internet’, ‘speed’, ‘increased’, ‘hacking’, ‘dns’]
          5. POS Tagging</li>
              <li>POS tagging refers to categorizing words in a corpus corresponding to a particular part of speech (eg: Noun, Verb, Adjective etc). This process helps improve the performance of the Lemmatizer.</li>
              <li>After POS Tagging: [(‘internet’, ‘a’), (‘speed’, ‘n’), (‘increased’, ‘v’), (‘hacking’, ‘v’), (‘dns’, ‘n’)]
          6. Lemmatization</li>
              <li>Lemmatization is grouping together different inflected forms of a word so that they can be analyzed as a single item.</li>
              <li>After Lemmatization: [‘internet’, ‘speed’, ‘increase’, ‘hack’, ‘dns’]</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Training:
    <ul>
      <li>For each training iteration, we input questions in a pairwise fashion - 𝑄<sub>𝑖</sub>, 𝑄<sub>𝑗</sub>.</li>
      <li>The model learns representations of building blocks of both sentences𝑄<sub>𝑖</sub>−&gt;ν<sub>𝑖</sub>, 𝑄<sub>𝑗</sub>−&gt;ν<sub>𝑗</sub>.</li>
      <li>Next, these representations are concatenated 𝑣<sub>𝑐𝑜𝑛</sub> = 𝑐𝑜𝑛𝑐𝑎𝑡(ν<sub>𝑖</sub>, ν<sub>𝑗</sub>), and passed on to a feedforward neural network or a machine learning model 𝐹 (𝑣𝑐𝑜𝑛) that predicts whether two questions are similar or not.</li>
      <li>∀ questions 𝑄<sub>𝑖</sub> ε question bank 𝑄<sub>𝐵</sub>, we group them into clusters 𝑐<sub><em>1</em></sub>,…,𝑐<sub><em>n</em></sub> for efficient inference.</li>
    </ul>

    <p><strong>Supervised method: Classifying sentences as similar or not using  BERT-based model (Added for midpoint report)</strong></p>
    <ul>
      <li>The tokenized version of the candidate sentence that we get from the preprocessing stage is first passed to a pre-trained BERT model.</li>
      <li>The output of this pre-trained BERT model is the sentence-level embedding of the tokenized version - E<sub>1.</sub>
</li>
      <li>The same process is done on the second candidate sentence to retrieve its sentence-level embedding - E<sub>2.</sub>
</li>
      <li>The two embeddings are concatenated, and then passed to a feed-forward neural network which consists of a fully connected layer, a ReLu layer, another fully       connected layer, and finally a softmax layer in the order specified.</li>
      <li>The softmax layer then outputs the probabilities of the pair of candidate questions being similar. Based on the probabilities we classify the questions as         similar or not similar.</li>
    </ul>

    <p><img src="https://user-images.githubusercontent.com/39705529/141838152-b27025fc-becc-47b6-a20f-f6c7bb41e548.png" alt="image"></p>

    <p><em>Fig 5: BERT takes in sentences as inputs and gives sentence-level embeddings as the output</em></p>
  </li>
  <li>Inference:
    <ul>
      <li>For a query question 𝑄<sub>𝑞</sub> we identify the cluster 𝑐<sub>𝑖</sub> it belongs to.</li>
      <li>For all candidate questions 𝑄<sub>𝑑</sub> belonging to cluster 𝑐<sub>𝑖</sub>, we find similarity 𝑠𝑖𝑚(𝑄<sub>𝑑</sub>, 𝑄<sub>𝑞</sub>). With clustering we avoid finding similarities with all questions in the question bank, making inference efficient.</li>
      <li>If for any 𝑄<sub>𝑑</sub> ε 𝑐<sub>𝑖</sub>, if 𝑠𝑖𝑚(𝑄<sub>𝑑</sub>, 𝑄<sub>𝑞</sub>) &gt; 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑, we flag that question as similar.</li>
      <li>We also output 𝑡𝑜𝑝 − 𝑘 similar questions based on 𝑠𝑖𝑚(𝑄<sub>𝑑</sub>, 𝑄<sub>𝑞</sub>).</li>
    </ul>
  </li>
  <li>Models in consideration:
    <ul>
      <li>Models for learning representations
        <ul>
          <li>BERT</li>
          <li>GPT</li>
          <li>XLNet</li>
        </ul>
      </li>
      <li>Models for clustering
        <ul>
          <li>K-means</li>
        </ul>
      </li>
      <li>Models for finding similarity
        <ul>
          <li>Feedforward neural network (FFN)</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><img src="https://yashjakhotiya.github.io/blog//images/2021-10-03-quora-question-pairs/similar-questions-flow-chart.png" alt="" title="Model in action"></p>

<h1 id="final-results-and-analysis">
<a class="anchor" href="#final-results-and-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Final results and analysis</h1>

<h3 id="supervised-learning-pipeline">
<a class="anchor" href="#supervised-learning-pipeline" aria-hidden="true"><span class="octicon octicon-link"></span></a>Supervised Learning Pipeline</h3>

<p><strong>Experiment 1 : Data Preprocessing and Augmentation Choices Ablation</strong></p>

<p>Through the first set of experiments, we try to observe the effects of data preprocessing and data augmentation on the performance of the model. We also apply three different types of models on a sample dataset of 1000 data points from the entire original dataset. The results of these are seen in the tables below:</p>

<table>
  <thead>
    <tr>
      <th>Data Preprocessing</th>
      <th>Data Augmentation</th>
      <th>Model</th>
      <th>Train accuracy</th>
      <th>Train F1 score</th>
      <th>Test accuracy</th>
      <th>Test F1 score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>No</td>
      <td>No</td>
      <td>Baseline</td>
      <td>0.57</td>
      <td>0.61</td>
      <td>0.42</td>
      <td>0.58</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>No</td>
      <td>Baseline</td>
      <td>0.61</td>
      <td>0.61</td>
      <td>0.57</td>
      <td>0.62</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>Yes</td>
      <td>Baseline</td>
      <td>0.6</td>
      <td>0.62</td>
      <td>0.61</td>
      <td>0.6</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>Data Preprocessing</th>
      <th>Data Augmentation</th>
      <th>Model</th>
      <th>Train accuracy</th>
      <th>Train F1 score</th>
      <th>Test accuracy</th>
      <th>Test F1 score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>No</td>
      <td>No</td>
      <td>Static BERT</td>
      <td>0.66</td>
      <td>0.61</td>
      <td>0.57</td>
      <td>0.53</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>No</td>
      <td>Static BERT</td>
      <td>0.67</td>
      <td>0.62</td>
      <td>0.59</td>
      <td>0.57</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>Yes</td>
      <td>Static BERT</td>
      <td>0.66</td>
      <td>0.62</td>
      <td>0.59</td>
      <td>0.55</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>Data Preprocessing</th>
      <th>Data Augmentation</th>
      <th>Model</th>
      <th>Train accuracy</th>
      <th>Train F1 score</th>
      <th>Test accuracy</th>
      <th>Test F1 score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>No</td>
      <td>No</td>
      <td>Fine-tuned BERT</td>
      <td>0.98</td>
      <td>0.97</td>
      <td>0.57</td>
      <td>0.41</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>No</td>
      <td>Fine-tuned BERT</td>
      <td>0.97</td>
      <td>0.97</td>
      <td>0.58</td>
      <td>0.5</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>Yes</td>
      <td>Fine-tuned BERT</td>
      <td>0.97</td>
      <td>0.96</td>
      <td>0.64</td>
      <td>0.52</td>
    </tr>
  </tbody>
</table>

<p>Here, the baseline model is nothing but the pre-trained BERT-based model, and the embeddings of the sentences obtained from this model are compared using a cosine similarity. This provided us a good baseline result and starting point, which we tried to improve upon.</p>

<p>For the other two models, the dataset was then split into training and testing data with a ratio of 80:20 respectively. The models were trained over this data for 5 epochs with a learning rate of 0.0001 and batch size of 8. The architecture used for these models in this experiment : <strong><em>BERT -&gt; FFN -&gt; Sigmoid layer -&gt; FFN -&gt; Softmax layer</em></strong>. In the Static BERT model the pre-trained BERT model is kept static and the remaining layers are trained, whereas in the Fine-tuned BERT, all the layers are trained and the pre-trained BERT-based model is fine-tuned on our dataset.</p>

<p><strong>Inference</strong> : As we can see from the table, applying data preprocessing techniques (mentioned in section 2) and data augmentation techniques increases the accuracy on the test data. This empirically backs our decision to do data preprocessing and augmentation.</p>

<p><strong>Experiment 2 : Fine Tuning Ablation</strong></p>

<p>Through the second set of experiments, we tried to gain insight on which model would work best for our problem. As a part of this, we ran our baseline, Static BERT, and Fine-tuned BERT on a sample dataset of 10000 datapoints randomly chosen from the original dataset. For the Static BERT and Fine-tuned BERT models we again split the data into train and test with a 80:20 ratio, and ran them for 10 epochs with a learning rate of 0.0001 and batch size 32. The architecture used for these 2 models in this experiment : <strong><em>BERT -&gt; FFN -&gt; PRelu -&gt; FFN -&gt; PRelu -&gt; FFN -&gt; Softmax Layer</em></strong>. The results are seen in the table below:</p>

<table>
  <thead>
    <tr>
      <th>Data Preprocessing</th>
      <th>Data Augmentation</th>
      <th>Model</th>
      <th>Train accuracy</th>
      <th>Train F1 score</th>
      <th>Test accuracy</th>
      <th>Test F1 score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Yes</td>
      <td>Yes</td>
      <td>Baseline</td>
      <td>0.59</td>
      <td>0.6</td>
      <td>0.6</td>
      <td>0.61</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>Yes</td>
      <td>Static BERT</td>
      <td>0.7</td>
      <td>0.63</td>
      <td>0.68</td>
      <td>0.61</td>
    </tr>
    <tr>
      <td>Yes</td>
      <td>Yes</td>
      <td>Fine-tuned BERT</td>
      <td>0.99</td>
      <td>0.98</td>
      <td>0.71</td>
      <td>0.62</td>
    </tr>
  </tbody>
</table>

<p><strong>Inference</strong> : As it can be seen in the table, the Fine-tuned BERT model produces the best accuracy results on the data as compared to the baseline and Static-BERT. This again empirically backs our decision to choose a pre-trained BERT model and fine-tune it on our dataset.</p>

<p><strong>Experiment 3 : Model Selection Ablation</strong></p>

<p>We chose to fine-tune our models based on the results from experiment 2. Next, we did some hyperparamter tuning to pick the best combination of hyperparameters for performance. We found that a batch size of 128 and a training over 5 epochs gave the best test results for our Fine-tuned BERT model. We then trained all 400,000 question pairs with this hyperparameter setting. We repeated the same process with GPT and XLNet architectures. The results for this experiment are shown below:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Train accuracy</th>
      <th>Test accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Fine-tuned BERT</td>
      <td>0.94</td>
      <td>0.86</td>
    </tr>
    <tr>
      <td>Fine-tuned GPT-2</td>
      <td>0.65</td>
      <td>0.63</td>
    </tr>
    <tr>
      <td>Fine-tuned XLNet</td>
      <td>0.64</td>
      <td>0.64</td>
    </tr>
  </tbody>
</table>

<p><strong>Inference</strong> : As it can be seen in the table, the Fine-tuned BERT model produces the best accuracy results on the test data as compared to the GPT-2 and XLNet models. Thus, we present the Fine-tuned BERT model as our final supervised learning model with a test accuracy of 86%.</p>

<h3 id="unsupervised-learning-pipeline-for-efficient-inference">
<a class="anchor" href="#unsupervised-learning-pipeline-for-efficient-inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Unsupervised Learning Pipeline for Efficient Inference</h3>

<p>We build an unsupervised learning pipeline in order to cluster similar questions. The pipeline consists of 3 steps :</p>
<ol>
  <li>The first part of the pipeline consists of creating BERT embeddings of all the questions after preprocessing them.</li>
  <li>
    <p>We then perform Principal Component Analysis (PCA) in order to reduce the dimensionality of our dataset. From the below graph, we can infer that with just 15 components, 99 percent of the variance is retained.</p>

    <p><img src="https://user-images.githubusercontent.com/46374506/144763989-c6abfce0-3003-4b3d-9921-d9aad90937f5.png" alt="image"></p>
  </li>
  <li>
    <p>We cluster the preprocessed dataset using Kmeans clustering algorithm. The elbow method plotted below, is used to find out the ideal number of clusters after analysing the tradeoff between performance and computation cost. From the graph, we can infer that the elbow is at 100 clusters</p>

    <p><img src="https://user-images.githubusercontent.com/46374506/144764219-06373665-f8cb-429a-a9e6-fced14b99b33.png" alt="image"></p>
  </li>
</ol>

<p>This unsupervised learning pipeline is used to create a smaller question space from which we can choose the candidate reference questions. This in turn reduces the time overhead by 100 times during inference. Here is an example illustrating potential canonical questions:<br><br></p>

<p><img src="https://user-images.githubusercontent.com/46374506/144765011-a35e1759-c413-47be-aa07-d9246e30c682.png" alt="image">
<br><br></p>

<h1 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h1>

<p>In this project, we propose a method to classify questions as similar or not similar using a combination of a supervised method and unsupervised method.</p>

<p>For the supervised method, we implemented multiple transformer based models (such as BERT, GPT-2, and XLNet) as backbones over a feed forward neural network to classify whether two questions are similar or not. After extensive results and ablations we were able to finalize our model as the pre-trained BERT model which is fine-tuned on our data. This supervised model classifies the given query question as similar or not after comparison with the top K reference questions obtained from the unsupervised pipeline based on their sentence embeddings. We achieve a test accuracy of 86% for this task.</p>

<p>For the unsupervised method, we use K-means clustering to group the questions in the dataset based on their sentence level embeddings, in order to narrow down the question space from which the reference questions are picked for comparison. The unsupervised pipeline gives us the candidate similar questions for each of the clusters. Through this, we were able to form 100 clusters which reduces our overhead significantly.</p>

<h1 id="future-experiments">
<a class="anchor" href="#future-experiments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Future experiments</h1>

<p>Future work may include downstream tasks such as automatic question tagging, and personalized recommendation of questions based on the field of interest. This can be very advantageous for forums such as Ed, Quora, Piazza, etc. and can help towards making the work of teaching staff easier.</p>

<h1 id="timeline-and-responsibilities">
<a class="anchor" href="#timeline-and-responsibilities" aria-hidden="true"><span class="octicon octicon-link"></span></a>Timeline and Responsibilities</h1>
<p><img src="https://yashjakhotiya.github.io/blog/images/2021-10-03-quora-question-pairs/timeline.png" alt="" title="Timeline"></p>

<h1 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h1>
<ol>
  <li>https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs</li>
  <li>D. A. Prabowo and G. Budi Herwanto, “Duplicate Question Detection in Question Answer Website using Convolutional Neural Network,” 2019 5th International Conference on Science and Technology (ICST), 2019, pp. 1-6, doi: 10.1109/ICST47872.2019.9166343.</li>
  <li>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina
Toutanova, “BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding”, arXiv:1810.</li>
  <li>Tianqi Chen andCarlos Guestrin.2016.XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ‘16). Association for Computing Machinery, New York,NY,USA,785–794.DOI:https://doi.org/10.1145/2939672.785.</li>
  <li>Reynolds D.(2009)GaussianMixture Models. In:Li S.Z.,Jain A. (eds) Encyclopedia of Biometrics. Springer, Boston, MA. https://doi.org/10.1007/978-0-387-73003-5_</li>
</ol>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="yashjakhotiya/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/nlp/2021/10/03/quora-question-pairs.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/yashjakhotiya" target="_blank" title="yashjakhotiya"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/yash-jakhotiya" target="_blank" title="yash-jakhotiya"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/yash_jakhotiya" target="_blank" title="yash_jakhotiya"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
