{
  
    
        "post0": {
            "title": "Can we avoid repeated questions by identifying similar intent?",
            "content": "Introduction/Background : . With public discussion forums becoming highly popular in recent times, especially with classes and work moving to an online setup, websites like Edstem, Piazza, Stack Overflow and Quora have a constant influx of a large volume of questions. Multiple questions with the same intent leads to redundancy, and waste of time, space and resources. . Problem Definition : . The aim is to identify and flag questions with a high similarity index, and retain only canonical questions in order to make the work of administrative staff such as TAs/Professors easier in terms of answering repeated questions or questions of the same intent. . Methods: . Dataset: The dataset that we will be using is the Quora Question Pairs dataset[1]. It consists of 404,290 pairs of questions. Each datapoint consists of a pairof questions and whether or not they are similar. . | Data preprocessing: One augmentation method we plan to leverage is the transitive property of similarity and create more question pairs. Assuming a question is represented as 𝑄𝑖. If 𝑄1 - 𝑄2 are similar and 𝑄2 - 𝑄3 are similar, then 𝑄1 - 𝑄3 will also be similar. | Other preprocessing techniques we intend to use are, Noise Removal | Removing stopwords and punctuation | Tokenization | Lemmatization and stemming | | . | Training: For each training iteration, we input questions in a pairwise fashion - 𝑄𝑖, 𝑄𝑗. | The model learns representations of building blocks of both sentences𝑄𝑖−&gt;ν𝑖, 𝑄𝑗−&gt;ν𝑗. | Next, these representations are concatenated 𝑣𝑐𝑜𝑛 = 𝑐𝑜𝑛𝑐𝑎𝑡(ν𝑖, ν𝑗), and passed on to a feedforward neural network or a machine learning model 𝐹 (𝑣𝑐𝑜𝑛) that predicts whether two questions are similar or not. | ∀ questions 𝑄𝑖 ε question bank 𝑄𝐵, we group them into clusters 𝑐1,…,𝑐n for efficient inference. | . | Inference: For a query question 𝑄𝑞 we identify the cluster 𝑐𝑖 it belongs to. | For all candidate questions 𝑄𝑑 belonging to cluster 𝑐𝑖, we find similarity 𝑠𝑖𝑚(𝑄𝑑, 𝑄𝑞). With clustering we avoid finding similarities with all questions in the question bank, making inference efficient. | If for any 𝑄𝑑 ε 𝑐𝑖, if 𝑠𝑖𝑚(𝑄𝑑, 𝑄𝑞) &gt; 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑, we flag that question as similar. | We also output 𝑡𝑜𝑝 − 𝑘 similar questions based on 𝑠𝑖𝑚(𝑄𝑑, 𝑄𝑞). | . | Models in consideration: Potential models for learning representations BERT-like models (RoBERTa, ALBERT, DeBERTa) | GPT models | XLNet | . | Potential models for clustering K-means | DBSCAN | GMM | . | Potential models for finding similarity Feedforward neural network | Gradient boosted trees | . | . | . Potential Results and Discussions: . We aim to confidently identify a representative question for a query question input by a user, and further list the top-k most relevant questions. . We also hope to gain insight by clustering similar questions and analyze the reason for their similarity, which may help in downstream tasks such as automatic question tagging, and personalized recommendation of questions basedon the field of interest. . Proposed Timeline and Responsibilities : . . References : . https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs | D. A. Prabowo and G. Budi Herwanto, “Duplicate Question Detection in Question Answer Website using Convolutional Neural Network,” 2019 5th International Conference on Science and Technology (ICST), 2019, pp. 1-6, doi: 10.1109/ICST47872.2019.9166343. | Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”, arXiv:1810. | Tianqi Chen andCarlos Guestrin.2016.XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ‘16). Association for Computing Machinery, New York,NY,USA,785–794.DOI:https://doi.org/10.1145/2939672.785. | Reynolds D.(2009)GaussianMixture Models. In:Li S.Z.,Jain A. (eds) Encyclopedia of Biometrics. Springer, Boston, MA. https://doi.org/10.1007/978-0-387-73003-5_ |",
            "url": "https://yashjakhotiya.github.io/blog/nlp/2021/10/03/quora-question-pairs.html",
            "relUrl": "/nlp/2021/10/03/quora-question-pairs.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Containers, Container Runtimes, and What Kubernetes 'Docker' Deprecation Really Means",
            "content": "Containers? Aren’t they … like some sort of virtual machines? . Docker popularized the notion of using containers - isolated environments leveraging OS-level virtualization where each running process sees the environment as one whole computer. This seems awfully similar to virtual machines, except that it isn’t. Containers differ from virtual machines in that each container does not host the entire operating system the way virtual machines do. . . Container images, which become running containers when instantiated, store the application code and any required dependencies mentioned in the image Dockerfile. When you don’t need to worry about dependencies, shipping applications from a developer’s laptop to production servers or public cloud environments becomes easier. You could package your application as a custom built VM image relying on a fully functional traditional OS packaged with it, but container images are much more lightweight and can be easily maintained. . But, every dockerfile I see ultimately stems from an OS image. Doesn’t that mean container images have their own OS installed? . This is an excellent question. Thanks for asking! To answer this, let us get some background context. . In most modern OS, an application process runs in what is known as a user mode. The idea here is to restrict the memory area accessible to an application process and prevent it from accessing and potentially corrupting memory areas associated with kernel and other application processes. This is implemented using Virtual memory and Protection Rings, and assisted by hardware in the form of Protected mode and Memory Management Units. . Providing fault tolerance and computer security with this form of memory protection effectively results in a typical OS being divided into two bifurcations - a user space and a kernel space. An application running in user space can access resources which it does not have direct access to (like I/O devices or files lying on a disk) with special requests to the kernel called system calls. . . System calls serve as APIs for all userland software to interact with the kernel. In the Linux world, all distros (a bit simplification here) run the same kernel. This makes it possible for the userland software coming from Ubuntu to talk to a CentOS kernel. . What you see inside a Dockerfile, which gets installed in the built image, is not a full-fledged OS. It is the trimmed-down version of the userland software of the OS, bare enough to talk to the host’s kernel. It is not uncommon to see containers with Ubuntu, CentOS, and Debian base run parallely on a RHEL7 host. . Ok. I am curious. How is this implemented? . To be honest, container implementation recipe is really not that difficult if you understand its three main ingredients - cgroups, namespaces and chroot. Let us focus on each of them below. . cgroups, or Control Groups is a Linux kernel feature. With cgroups you can allocate, monitor, and limit resources - like CPU time, memory, or network bandwidth - to a process or a collection of processes. Linux command cgcreate helps you create a control group, cgset sets resource limits for the control group, and with cgexec you can run a command in the control group. . | A namespace is another Linux kernel feature, with which you can isolate a global resource. This creates an illusion of a separate instance of the resource to the processes running in the namespace, and any changes made are not visible outside! The resources you can abstract this way include process IDs, hostnames, user IDs, etc. unshare [options] [program [arguments]] is a Linux utility you can use to create namespaces (supplied in options) and run program in it. For example you can create a UTS (Unix Time Sharing) namespace, which controls host and domain names, using the -u option as illustrated below. &gt; hostname # show current hostname personal-ubuntu &gt; unshare -u /bin/sh # run a shell instance with UTS namespace unshared from parent &gt; hostname a-different-hostname # change hostname to a-different-hostname &gt; hostname # verify that the hostname has been changed a-different-hostname &gt; exit # exit from the shell process, effectively destroying the namespace &gt; hostname # voila! personal-ubuntu # changing the hostname inside the namespace has no effect outside! . | chroot is a Linux utility that can change the apparent root directory for a process and its children. Running chroot NEWROOT command will run command with NEWROOT as its apparent root directory. This modified environment is also called a chroot jail, because command can not name and hence can not normally access files outside NEWROOT. | Now that you have understood these three main concepts, let us create a container image from the following dockerfile, which we want to run. . FROM ubuntu:18.04 COPY script.py /app CMD python /app/script.py . This container image contains the ubuntu:18.04 userland file structure heirarchy, /app/script.py and some environment configuration. Ignoring the config part for now, your minimal implementation can run this image in just 4 steps. . Export and extract contents of the image in new_root_dir &gt; mkdir new_root_dir &gt; docker export docker_image | tar -xf - -C new_root_dir . | Create a control group and set memory and CPU use limits &gt; control_group=$(uuidgen) &gt; cgcreate -g cpu,memory:$control_group &gt; cgset -r memory.limit_in_bytes=50000000 $control_group &gt; cgset -r cpu.shares=256 $control_group . | Executing inside the control group, call unshare to separate namespaces and execute script.py inside the new_root_dir jail &gt; cgexec -g cpu,memory:$control_group unshare -uinpUrf --mount-proc sh -c &quot;chroot new_root_dir /app/script.py&quot; . | Cleanup. Delete the cgroup and new_root_dir. Unless bound to a file, namespaces cease to exist once all running processes in the namespace have exited. &gt; cgdelete -r -g cpu,memory:$control_group &gt; rm -r new_root_dir . | Lo and behold! You have just created a minimal container runtime! . Whoa! Wait, Container R… what? . Container Runtime - the code and tooling responsible for running containers. What you created above is the heart of what every container runtime does. Although, it catches the essence of container runtimes, it’s still minimal. Docker images also have something known as a config.json. This file has, among other things, environment variables to be set for the running process inside the container, and the uid and gid of the user the process must run as. . The code to run containers used to be deep inside a monolith called Docker. But, it need not be. As long as vendors agree upon a common specification for images and a common specification for runtimes, anybody could create runtimes customized to their needs. That’s exactly what they did. Docker, CoreOS, Google and other industry leaders in the container space came together and launched Open Container Initiative in June 2015. OCI is responsible for defining image-spec and runtime-spec, which every OCI-compliant image builder and container runtime has to abide by. . OCI even develops and maintains a reference implementation of the runtime-spec called runc. runc broke off from Docker, as part of the Open Container Initiative. Although, runc is self-sufficient to run containers, it is a low-level runtime. The only developers that work with runc are developers of high-level runtimes. . Come on! These container runtimes have ‘levels’ now? . Yes, they very much do! If you ever used Docker, you might know that running containers from images isn’t all that you do. You might want to pull images from registries before you actually run them. A higher level runtime does that for you. . . Higher level runtimes are also responsible for unpacking the container image into an OCI runtime bundle before spawning a runc process to run it. In addition to managing the lifecycle of a container, higher level runtimes are also sometimes responsible for low level storage and network namespace management. This is usually in place to facilitate interaction between individual container processes. . Humans aren’t the only entities that interact with higher level runtimes. Container orchestration services (just a fancy term for management and configuration of containers across large dynamic systems), like Kubernetes, need to interact with high-level runtimes. For most industry use-cases, it’s less humans and more such services that talk to these runtimes. . Did you mention Kubernetes? You had my curiosity. Now you have my attention. . What interacts with high-level container runtimes are not client-facing modules of a running Kubernetes instance, but a node-agent called kubelet which runs on all nodes in a Kubernetes cluster. Kubelet is responsible to ensure all containers mentioned in a pod’s specification are running and healthy. It registers nodes, sends pod status and events, and reports resource utilization higher up the command chain. . With the introduction of OCI, many container runtimes came up that supported running OCI-compliant container images, and so arised the need for Kubernetes to support multiple runtimes. To avoid deep integration of such runtimes into kubelet source code, and the subsequent maintenance that would follow, Kubernetes introduced the Container Runtime Interface - an interface definition which enables kubelet to use a wide variety of runtimes. It is the responsibility of a container runtime to implement this interface as an internal package or as a shim. . containerd, a prominent high-level container runtime, which broke off from Docker similar to runc, recently merged its separate cri-plugin codebase to its main containerd/containerd repository, marking CRI-implementation to be an important part of the container runtime. cri-o is another implementation of CRI, focused and optimized only for Kubernetes, and, unlike containerd, can not service docker daemons for container orchestration. . . Now that we have established CRI, let us talk about what the recent Kubernetes Docker Deprecation really means. . Finally! . Kuberenetes recently announced that it would be deprecating Docker. It really isn’t as dramatic as it sounds. What Kuberenetes will not support is Docker as a runtime, and nothing else changes. Images built with dockerfiles are OCI-compliant and hence can be very well used with Kubernetes. Both containerd and cri-o know how to pull them, and runc knows how to run them. . Docker, being built for human interaction, isn’t really friendly for Kubernetes as just a runtime. To interact with it, Kubernetes has to develop a module called dockershim, which implements CRI support for Docker. This makes Docker call-able by kubelet as a runtime. Kubernetes is no longer willing to maintain this, especially when containerd (which Docker internally uses) has a CRI plugin. If you are developer, you do not really need to worry about what runtimes kubelet can interact with. Docker built images are perfectly fine for Kubernetes to consume! . . End Notes . I hope you liked reading this blog post as much as I loved writing it. I’ll soon update a large list of references which can be used for further reading. In the meanwhile, please feel free to follow me on Twitter and subscribe to the Blog’s RSS Feed for further updates. For any feedback or suggestions for blog posts, please drop an email or DM on Twitter. Thanks for reading! .",
            "url": "https://yashjakhotiya.github.io/blog/containers/kubernetes/2020/12/20/container-runtimes.html",
            "relUrl": "/containers/kubernetes/2020/12/20/container-runtimes.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "GSoC '20: Kubeflow Customer User Journey Notebooks with Tensorflow 2.x Keras",
            "content": "Introduction . Open source software development and Google Summer of Code, both started long before the summer of 2020. When the world was starting to grapple with the realities of remote work, open source community was already thriving on it. Over the course of my college years, I have found out three things that I am passionate about - open source, machine learning and SRE. Kubeflow has managed to incorporate all of these into one and doing a project with this organisation has been a dream come true! . Goal . Kubernetes is already an industry-standard in managing cloud resources. Kubeflow is on its path to become an industry standard in managing machine learning workflows on cloud. Examples that illustrate Kubeflow functionalities using latest industry technologies make Kubeflow easier to use and more accessible to all potential users. This project has aimed at building samples for Jupyter notebook to Kubeflow deployment using Tensorflow 2.0 Keras for backend training code, illustrating customer user journey (CUJ) in the process. This project has also served as an hands-on to large scale application of machine learning bringing in the elements of DevOps and SRE and this has kept me motivated throughout the project. . The Kubeflow Community . The Kubeflow community is a highly approachable and closely-knit community that has been reaching out to and helping potential GSoC students well before the application period. Respecting this, I made sure to take feedback for my proposal of the project idea I chose, before the application deadline. Mentors Yuan Tang, Ce Gao and Jack Lin were candid in providing me feedback and I refined and changed my proposal accordingly. To my sweet surprise, I got selected for the idea!😁 What has really helped me in these three months of coding period is that one month of community bonding where I got to know the community and more about the technicalities of Kubeflow. . The Project . . Examples created as part of this project needed to be easily reproducible to serve their purpose. Initially the underlying model decided to demonstrate Kubeflow functionalities was a BiDirectional RNN to be trained on IMDB large movie review dataset for sentiment analysis based on a tensorflow tutorial. Over the course of time, we decided to also add another set of examples using a neural machine translation model in its backend trained on a Spanish to English dataset based on another tensorflow tutorial. . The reasons for choosing these models were: . The kubeflow/examples repo needed more NLP-related tasks. | These were more of hello world tasks in the field of NLP. So that users who go through these samples need not worry about training code and focus more on Kubeflow’s functionalities. | These are based on tensorflow tutorials. Kubeflow tutorials based on Tensorflow tutorials show better coupling between the two. | . Repository Structure . I created a repo under my own profile to regularly push commits to and my mentors consistently reviewed the work I pushed there. This repo has all of my work with the log history preserved. Each of the two models has the following folder structure explaining core Kubeflow functionalities - . &lt;training-model&gt;.py - This is the core training code upon which all subsequent examples showing Kubeflow functionalities are based. Please go through this first to know more about the machine learning task subsequent notebooks will manage. For example check the source code of the model used for the text classification task. . | distributed_&lt;training-model&gt;.py - To truly take advantage of multiple compute nodes, the training code has to be modified to support distributed training. The code in the above mentioned file is modified with Tensorflow’s distributed training strategy and hosted here. . | Dockerfile - This is the dockerfile which is used to build Docker image of the training code. Some Kubeflow functionalities require that a docker image of the training code is built and hosted on a docker container registry. This Docker 101 tutorial is a good starting point to get hands-on training on Docker. For complete starters in the field of containerization, this introduction can serve as a good starting point. The dockerfile used with the source code mentioned above can be found here. . | fairing-with-python-sdk.ipynb - Fairing is a Kubeflow functionality that lets you run model training tasks remotely. This is the Jupyter notebook which deploys a model training task on cloud using Kubeflow Fairing. Fairing does not require you to build a Docker image of the training code first. Hence, its training code resides in the same notebook. To know more about Kubeflow Fairing, please visit Fairing’s official documentation. To get a better idea about Fairing, you can take a look at the text classification Fairing notebook here. . | katib-with-python-sdk.ipynb - Katib is a Kubeflow functionality that lets you perform hyperparameter tuning experiments and reports best set of hyperparameters based on a provided metric. This is the Jupyter notebook which launches Katib hyperparameter tuning experiments using its Python SDK. Katib requires you to build and host a Docker image of your training code in a container registry. For this sample, we have used gcloud builds to build the required Docker image of the training code along with the training data and host it on Google Container Registry. This is the notebook we used to demonstrate Katib for the text classification task. . | tfjob-with-python-sdk.ipynb - TFJobs are used to run distributed training jobs over Kubernetes. With multiple workers, TFJob truly leverage the ability of your code to support distributed training. This Jupyter notebook demonstrates how to use TFJob. The Docker image built from the distributed version of our core training code is used in this notebook. TFJob notebook for the text classification task can be found here . | tekton-pipeline-with-python-sdk.ipynb - Kubeflow Pipeline is a platform that lets you build, manage and deploy end-to-end machine learning workflows. This is a Jupyter notebook which bundles Katib hyperparameter tuning and TFJob distributed training into one Kubeflow pipeline. The pipeline used here uses Tekton in its backend. Tekton is a Kubernetes resource to create efficient continuous integration and delivery (CI/CD) systems. The pipeline notebook for the text classification task can be found at this place. . | Merged PRs . In the community bonding period, I opened numerous small issues and PRs solving these issues, as I encountered them when reading documentation or implementing an example. This was done in an effort to know more about the Kubeflow community. . For the main project, I copied these built notebooks and the final work product into a directory created in my fork of the kubeflow/examples repo and created a PR to add these notebooks in Kubeflow’s official repo. The PR got merged and the code currently resides in kubeflow/examples/tensorflow_cuj directory marking the completion of the project. . Special Thanks . Special thanks are due to - . My mentors for their valuable guidance throughout the project. | Jeremy and Sarah for smooth conduction of the Kubeflow GSoC program. | The GSoC Discord Server and the GSoC Telegram Channel for the help, casual talks and a strong global student community. | .",
            "url": "https://yashjakhotiya.github.io/blog/open-source/mlops/2020/08/23/gsoc-kubeflow.html",
            "relUrl": "/open-source/mlops/2020/08/23/gsoc-kubeflow.html",
            "date": " • Aug 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://yashjakhotiya.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yashjakhotiya.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}